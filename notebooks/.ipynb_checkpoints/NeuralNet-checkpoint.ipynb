{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from matplotlib.pylab import rcParams\n",
    "rcParams['figure.figsize'] = 16, 8\n",
    "\n",
    "from scipy.stats import norm\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from datetime import datetime\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.neural_network import MLPClassifier  \n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import SCORERS\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>project_id</th>\n",
       "      <th>blurb</th>\n",
       "      <th>category</th>\n",
       "      <th>country</th>\n",
       "      <th>created_at</th>\n",
       "      <th>currency</th>\n",
       "      <th>deadline</th>\n",
       "      <th>fx_rate</th>\n",
       "      <th>goal</th>\n",
       "      <th>launched_at</th>\n",
       "      <th>name</th>\n",
       "      <th>staff_pick</th>\n",
       "      <th>location</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>project_url</th>\n",
       "      <th>reward_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4938</th>\n",
       "      <td>KS_104938</td>\n",
       "      <td>NaN</td>\n",
       "      <td>art</td>\n",
       "      <td>US</td>\n",
       "      <td>1509679461</td>\n",
       "      <td>USD</td>\n",
       "      <td>1515800048</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>40000.0</td>\n",
       "      <td>1510616048</td>\n",
       "      <td>awlsies</td>\n",
       "      <td>False</td>\n",
       "      <td>Los Angeles, CA</td>\n",
       "      <td>digital art</td>\n",
       "      <td>https://www.kickstarter.com/projects/145490711...</td>\n",
       "      <td>https://www.kickstarter.com/projects/145490711...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5128</th>\n",
       "      <td>KS_105128</td>\n",
       "      <td>Wildlife Art to empower disadvantaged children...</td>\n",
       "      <td>art</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1476524391</td>\n",
       "      <td>EUR</td>\n",
       "      <td>1479305419</td>\n",
       "      <td>1.245664</td>\n",
       "      <td>2500.0</td>\n",
       "      <td>1476709819</td>\n",
       "      <td>ArtConnects Etosha</td>\n",
       "      <td>True</td>\n",
       "      <td>Windhoek, Namibia</td>\n",
       "      <td>painting</td>\n",
       "      <td>https://www.kickstarter.com/projects/701001795...</td>\n",
       "      <td>https://www.kickstarter.com/projects/701001795...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16081</th>\n",
       "      <td>KS_116081</td>\n",
       "      <td>Create a treehouse as an undisturbed, protecte...</td>\n",
       "      <td>design</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1440609127</td>\n",
       "      <td>EUR</td>\n",
       "      <td>1449650435</td>\n",
       "      <td>1.245664</td>\n",
       "      <td>21500.0</td>\n",
       "      <td>1447317635</td>\n",
       "      <td>Treehouse Namibia - Baumhaus Namibia</td>\n",
       "      <td>False</td>\n",
       "      <td>Okahandja, Namibia</td>\n",
       "      <td>architecture</td>\n",
       "      <td>https://www.kickstarter.com/projects/102468866...</td>\n",
       "      <td>https://www.kickstarter.com/projects/102468866...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21538</th>\n",
       "      <td>KS_121538</td>\n",
       "      <td>Own a pair of the original bushman San-dals, d...</td>\n",
       "      <td>fashion</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1462284723</td>\n",
       "      <td>GBP</td>\n",
       "      <td>1465461297</td>\n",
       "      <td>1.401112</td>\n",
       "      <td>70000.0</td>\n",
       "      <td>1462869297</td>\n",
       "      <td>THE ORIGINAL SAN-DAL</td>\n",
       "      <td>True</td>\n",
       "      <td>Tsumkwe, Namibia</td>\n",
       "      <td>footwear</td>\n",
       "      <td>https://www.kickstarter.com/projects/vivobaref...</td>\n",
       "      <td>https://www.kickstarter.com/projects/vivobaref...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58859</th>\n",
       "      <td>KS_158859</td>\n",
       "      <td>BRASIL is a photography book that captures the...</td>\n",
       "      <td>photography</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1437830095</td>\n",
       "      <td>USD</td>\n",
       "      <td>1445896740</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>17000.0</td>\n",
       "      <td>1443258097</td>\n",
       "      <td>BRASIL         a photobook</td>\n",
       "      <td>False</td>\n",
       "      <td>Windhoek, Namibia</td>\n",
       "      <td>photobooks</td>\n",
       "      <td>https://www.kickstarter.com/projects/kristinca...</td>\n",
       "      <td>https://www.kickstarter.com/projects/kristinca...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      project_id                                              blurb  \\\n",
       "4938   KS_104938                                                NaN   \n",
       "5128   KS_105128  Wildlife Art to empower disadvantaged children...   \n",
       "16081  KS_116081  Create a treehouse as an undisturbed, protecte...   \n",
       "21538  KS_121538  Own a pair of the original bushman San-dals, d...   \n",
       "58859  KS_158859  BRASIL is a photography book that captures the...   \n",
       "\n",
       "          category country  created_at currency    deadline   fx_rate  \\\n",
       "4938           art      US  1509679461      USD  1515800048  1.000000   \n",
       "5128           art     NaN  1476524391      EUR  1479305419  1.245664   \n",
       "16081       design     NaN  1440609127      EUR  1449650435  1.245664   \n",
       "21538      fashion     NaN  1462284723      GBP  1465461297  1.401112   \n",
       "58859  photography     NaN  1437830095      USD  1445896740  1.000000   \n",
       "\n",
       "          goal  launched_at                                  name  staff_pick  \\\n",
       "4938   40000.0   1510616048                               awlsies       False   \n",
       "5128    2500.0   1476709819                    ArtConnects Etosha        True   \n",
       "16081  21500.0   1447317635  Treehouse Namibia - Baumhaus Namibia       False   \n",
       "21538  70000.0   1462869297                  THE ORIGINAL SAN-DAL        True   \n",
       "58859  17000.0   1443258097            BRASIL         a photobook       False   \n",
       "\n",
       "                 location   subcategory  \\\n",
       "4938      Los Angeles, CA   digital art   \n",
       "5128    Windhoek, Namibia      painting   \n",
       "16081  Okahandja, Namibia  architecture   \n",
       "21538    Tsumkwe, Namibia      footwear   \n",
       "58859   Windhoek, Namibia    photobooks   \n",
       "\n",
       "                                             project_url  \\\n",
       "4938   https://www.kickstarter.com/projects/145490711...   \n",
       "5128   https://www.kickstarter.com/projects/701001795...   \n",
       "16081  https://www.kickstarter.com/projects/102468866...   \n",
       "21538  https://www.kickstarter.com/projects/vivobaref...   \n",
       "58859  https://www.kickstarter.com/projects/kristinca...   \n",
       "\n",
       "                                              reward_url  \n",
       "4938   https://www.kickstarter.com/projects/145490711...  \n",
       "5128   https://www.kickstarter.com/projects/701001795...  \n",
       "16081  https://www.kickstarter.com/projects/102468866...  \n",
       "21538  https://www.kickstarter.com/projects/vivobaref...  \n",
       "58859  https://www.kickstarter.com/projects/kristinca...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('KS_train_data.csv', delimiter=',')\n",
    "df_test = pd.read_csv('KS_test_data.csv', delimiter=';')\n",
    "# X = df.loc[:,'f1':'f100'].values\n",
    "# y = [ bool(y) for y in df.loc[:,'loss'].values ]\n",
    "df_test[df_test.isnull().any(1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['NA', 'SA', 'EU', 'AF', 'OC', 'AS', 'UNK', 'AT'], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.country = df.country.fillna('NA')\n",
    "df_test.country = df_test.country.fillna('NA')\n",
    "EU = ('GB', 'ES', 'FR', 'IT', 'NL', 'IS', 'CZ', 'FI', 'DE', 'IE', 'SJ', 'DK', 'SE', 'HU', 'NO', 'CY', 'CH', 'BE', \n",
    "          'LV', 'UA', 'AT', 'SI', 'LT', 'RO', 'RU', 'AX', 'MC', 'PT', 'GL', 'GR', 'SK', 'EE', 'BA', 'ME', 'LU', 'RS',\n",
    "         'PL', 'MD', 'BG', 'HR', 'MK', 'BY', 'XK', 'FO', 'MT')\n",
    "NA = ('US', 'CA', 'MX', 'CR', 'GT', 'HT', 'AG', 'JM', 'BZ', 'CU', 'SV', 'PR', 'PA', 'NI', 'DO', 'CW', 'VI', 'BB',\n",
    "         'HN', 'LC', 'TT', 'BS', 'GP', 'VC', 'DM')\n",
    "SA = ('AR', 'PE', 'SR', 'BR', 'BO', 'EC', 'CO', 'CL', 'VE', 'PY', 'GY', 'UY')\n",
    "AF = ('KE', 'MW', 'ZA', 'RW', 'LR', 'EG', 'SN', 'NG', 'TZ', 'GH', 'GQ', 'ZM', 'MG', 'ET', 'MA', 'CD', 'BF', 'UG',\n",
    "         'CI', 'DZ', 'ML', 'SD', 'ZW', 'CM', 'TN', 'NE', 'MZ', 'GN', 'SO', 'LY', 'DJ', 'GA', 'SS', 'GM', 'BJ', 'CF',\n",
    "          'CG', 'NA')\n",
    "AS = ('TH', 'ID', 'KH', 'IN', 'JP', 'TR', 'CN', 'MY', 'MN', 'IL', 'KR', 'PH', 'HK', 'SG', 'PS', 'TW', 'NP', 'IR',\n",
    "         'QA', 'VN', 'IQ', 'AE', 'LK', 'GE', 'LB', 'AM', 'KZ', 'AF', 'KP', 'BD', 'PK', 'MM', 'BT', 'JO', 'MV', 'LA',\n",
    "         'KW', 'SY', 'TJ', 'TL', 'YE', 'MO', 'KG')\n",
    "AT = ('AQ')\n",
    "OC = ('AU','NZ', 'PG', 'FJ', 'FM', 'CK', 'GU', 'NC', 'PF', 'VU' )\n",
    "UNK = ('?')\n",
    "def conditions(x):\n",
    "    if x in EU:\n",
    "        return \"EU\"\n",
    "    elif x in NA:\n",
    "        return \"NA\"\n",
    "    elif x in SA:\n",
    "        return \"SA\"\n",
    "    elif x in AF:\n",
    "        return \"AF\"\n",
    "    elif x in AS:\n",
    "        return \"AS\"\n",
    "    elif x in AT:\n",
    "        return \"AT\"\n",
    "    elif x in OC:\n",
    "        return \"OC\"\n",
    "    else:\n",
    "        return \"UNK\"\n",
    "\n",
    "func = np.vectorize(conditions)\n",
    "continents = func(df[\"country\"])\n",
    "df[\"continent\"] = continents\n",
    "\n",
    "continents = func(df_test['country'])\n",
    "df_test[\"continent\"] = continents\n",
    "df_test.continent.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_in_cat = {}\n",
    "funded_in_cat = {}\n",
    "rate_funded_cat = {}\n",
    "for x in df.category.unique():\n",
    "    total_in_cat[x] = df.loc[(df.category == x, 'project_id')].count()\n",
    "    funded_in_cat[x] = df.loc[(df.category == x) & (df.funded == True), 'project_id'].count() \n",
    "    rate_funded_cat[x] = funded_in_cat[x] / total_in_cat[x]\n",
    "df['rate_funded_cat'] = df.apply(lambda row: rate_funded_cat[row.category], axis=1)\n",
    "df_test['rate_funded_cat'] = df_test.apply(lambda row: rate_funded_cat[row.category], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_in_country = {}\n",
    "funded_in_country = {}\n",
    "rate_funded_country = {}\n",
    "for x in df.country.unique():\n",
    "    total_in_country[x] = df.loc[(df.country == x, 'project_id')].count()\n",
    "    funded_in_country[x] = df.loc[(df.country == x) & (df.funded == True), 'project_id'].count() \n",
    "    rate_funded_country[x] = funded_in_country[x] / total_in_country[x]\n",
    "df['rate_funded_country'] = df.apply(lambda row: rate_funded_country[row.country], axis=1)\n",
    "df_test['rate_funded_country'] = df.apply(lambda row: rate_funded_country[row.country], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_in_continent = {}\n",
    "funded_in_continent = {}\n",
    "rate_funded_continent = {}\n",
    "for x in df.continent.unique():\n",
    "    total_in_continent[x] = df.loc[(df.continent == x, 'project_id')].count()\n",
    "    funded_in_continent[x] = df.loc[(df.continent == x) & (df.funded == True), 'project_id'].count() \n",
    "    rate_funded_continent[x] = funded_in_continent[x] / total_in_continent[x]\n",
    "df['rate_funded_continent'] = df.apply(lambda row: rate_funded_continent[row.continent], axis=1)\n",
    "df_test['rate_funded_continent'] = df.apply(lambda row: rate_funded_continent[row.continent], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_in_sub = {}\n",
    "funded_in_sub = {}\n",
    "rate_funded_sub = {}\n",
    "for x in df.subcategory.unique():\n",
    "    total_in_sub[x] = df.loc[(df.subcategory == x, 'project_id')].count()\n",
    "    funded_in_sub[x] = df.loc[(df.subcategory == x) & (df.funded == True), 'project_id'].count() \n",
    "    rate_funded_sub[x] = funded_in_sub[x] / total_in_sub[x]\n",
    "df['rate_funded_sub'] = df.apply(lambda row: rate_funded_sub[row.subcategory], axis=1)\n",
    "df_test['rate_funded_sub'] = df.apply(lambda row: rate_funded_sub[row.subcategory], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['delta_time_created'] = df.deadline - df.created_at\n",
    "df['delta_time_launched'] = df.deadline - df.launched_at\n",
    "df['delta_time_launched_days'] = df.delta_time_launched / 86400\n",
    "df['delta_time_created_days'] = df.delta_time_launched / 86400\n",
    "df['goal_converted_log'] = np.log(df.goal * df.fx_rate)\n",
    "df['goal_per_day'] = df['goal_converted_log'] / df['delta_time_launched']\n",
    "cols = ['rate_funded_sub','rate_funded_continent', 'rate_funded_cat', \n",
    "        'delta_time_launched_days', 'goal_converted_log', 'staff_pick']\n",
    "\n",
    "df_test['delta_time_created'] = df_test.deadline - df.created_at\n",
    "df_test['delta_time_launched'] = df_test.deadline - df.launched_at\n",
    "df_test['delta_time_launched_days'] = df_test.delta_time_launched / 86400\n",
    "df_test['delta_time_created_days'] = df_test.delta_time_launched / 86400\n",
    "df_test['goal_converted_log'] = np.log(df.goal * df_test.fx_rate)\n",
    "df_test['goal_per_day'] = df_test['goal_converted_log'] / df_test['delta_time_launched']\n",
    "cols = ['rate_funded_sub','rate_funded_continent', 'rate_funded_cat', \n",
    "        'delta_time_launched_days', 'goal_converted_log', 'staff_pick']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = df\n",
    "new_df_test = df_test\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import scipy as sp\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.max_features = 500\n",
    "vectorizer.max_df = .5\n",
    "\n",
    "X = vectorizer.fit_transform(new_df.name.astype('U')) \n",
    "count_vect_df = pd.DataFrame(X.todense(), columns=vectorizer.get_feature_names())\n",
    "new_df = pd.concat([new_df, count_vect_df], axis=1)\n",
    "new_df_test = pd.concat([new_df_test, count_vect_df], axis=1)\n",
    "print(new_df_test.columns)\n",
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in vectorizer.get_feature_names():\n",
    "    cols.append(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[cols]\n",
    "y = df['funded']\n",
    "\n",
    "X_test_data = df_test[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contigency_matrix(true_y, predicted_y):\n",
    "    # YOUR CODE HERE, Create TP, FP, TN, FN\n",
    "    tp=fp=tn=fn=0\n",
    "    for true, pred in zip(true_y, predicted_y):\n",
    "        if pred == True:\n",
    "            if pred == true:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fp += 1\n",
    "        else:\n",
    "            if pred == true:\n",
    "                tn += 1\n",
    "            else:\n",
    "                fn += 1      \n",
    "    matrix = np.array(([tp, fp], [tn, fn]))\n",
    "    # Make sure your output fits the following format:\n",
    "    # matrix = np.array(([TP, FP], [TN, FN]))\n",
    "    return matrix\n",
    "\n",
    "def accuracy(true_y, predicted_y):\n",
    "    matrix = contigency_matrix(true_y, predicted_y)\n",
    "    tp = matrix[0][0]\n",
    "    fp = matrix[0][1]\n",
    "    tn = matrix[1][0]\n",
    "    fn = matrix[1][1]\n",
    "    if tp+fp+fn+tn == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        accuracy = (tp+tn)/(tp+fp+fn+tn)\n",
    "        return accuracy\n",
    "def precision(true_y, predicted_y):\n",
    "    matrix = contigency_matrix(true_y, predicted_y)\n",
    "    tp = matrix[0][0]\n",
    "    fp = matrix[0][1]\n",
    "    tn = matrix[1][0]\n",
    "    fn = matrix[1][1]\n",
    "    if tp+fp == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        precision = tp/(tp+fp)\n",
    "        return precision\n",
    "def recall(true_y, predicted_y):\n",
    "    matrix = contigency_matrix(true_y, predicted_y)\n",
    "    tp = matrix[0][0]\n",
    "    fp = matrix[0][1]\n",
    "    tn = matrix[1][0]\n",
    "    fn = matrix[1][1]\n",
    "    if tp+fn == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        recall = tp/(tp+fn)\n",
    "        return recall\n",
    "def f1(true_y, predicted_y):\n",
    "    precision_v = precision(true_y, predicted_y)\n",
    "    recall_v = recall(true_y, predicted_y)\n",
    "    if precision_v+recall_v == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        f1 = 2*((precision_v*recall_v)/(precision_v+recall_v))\n",
    "        return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial(X, degree):\n",
    "    \n",
    "    ### BEGIN SOLUTION\n",
    "    poly = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "    X_poly = poly.fit_transform(X)\n",
    "    ### END SOLUTION\n",
    "    return X_poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(y)\n",
    "y = y.reshape(-1,1)\n",
    "X_train,X_test,y_train,y_test = train_test_split(X, y, test_size=0.25)\n",
    "imp_median_X = SimpleImputer(missing_values=np.nan, strategy='median').fit(X_train)\n",
    "X_train = imp_median_X.transform(X_train)\n",
    "X_test = imp_median_X.transform(X_test)\n",
    "\n",
    "imp_median_y = SimpleImputer(missing_values=np.nan, strategy='median').fit(y_train)\n",
    "y_train = imp_median_y.transform(y_train)\n",
    "y_test = imp_median_y.transform(y_test)\n",
    "\n",
    "# fit scaler and scale features\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train) \n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    \n",
    "def compute_scores(X_train,X_test,y_train,y_test, C):\n",
    "    # fit logistic regression model\n",
    "    logreg = LogisticRegression(C=C, solver='liblinear').fit(X_train,y_train.ravel())\n",
    "    # predict y for train set\n",
    "    pred_train = logreg.predict(X_train).tolist()\n",
    "    # predict y for test set\n",
    "    pred_test = logreg.predict(X_test).tolist()\n",
    "            \n",
    "    # calculate evaluation measures\n",
    "    evaluation_measures = dict()\n",
    "    evaluation_measures['accuracy_train'] = accuracy(y_train, pred_train)\n",
    "    evaluation_measures['accuracy_test'] = accuracy(y_test, pred_test)\n",
    "    \n",
    "    evaluation_measures['precision_train'] = precision(y_train, pred_train)\n",
    "    evaluation_measures['precision_test'] = precision(y_test, pred_test)\n",
    "    \n",
    "    evaluation_measures['recall_train'] = recall(y_train, pred_train)\n",
    "    evaluation_measures['recall_test'] = recall(y_test, pred_test)\n",
    "    \n",
    "    evaluation_measures['f1_train'] = f1(y_train, pred_train)\n",
    "    evaluation_measures['f1_test'] = f1(y_test, pred_test)\n",
    "    \n",
    "    return evaluation_measures\n",
    "\n",
    "# for power in [1, 2]:\n",
    "#     X_train_poly = polynomial(X_train, power)\n",
    "#     X_test_poly = polynomial(X_test, power)\n",
    "# # Scale all features using the RobustScaler\n",
    "# scaler = RobustScaler().fit(X_train_poly)\n",
    "# X_train_scaled = scaler.transform(X_train_poly)\n",
    "# X_test_scaled = scaler.transform(X_test_poly)\n",
    "C = [1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 1e2, 1e3, 1e4]\n",
    "measures = pd.DataFrame()\n",
    "for c in C:\n",
    "    em = compute_scores(X_train_scaled,X_test_scaled,y_train,y_test, c)\n",
    "    em = pd.Series(em)\n",
    "    measures = measures.append(em, ignore_index=True)\n",
    "measures.index = C\n",
    "measures.index = measures.index.rename('C-value')\n",
    "display(measures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[36098,  7453],\n",
       "       [23642,  7807]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg = LogisticRegression(C=1, solver='liblinear').fit(X_train,y_train.ravel())\n",
    "pred_train = logreg.predict(X_train).tolist()\n",
    "contigency_matrix(y_train, pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.7919714285714285\n",
      "Testing accuracy: 0.7877\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "X_train,X_test,y_train,y_test = train_test_split(X, y, test_size=0.3)\n",
    "transformer = RobustScaler().fit(X_train)\n",
    "X_train = transformer.transform(X_train)\n",
    "X_test = transformer.transform(X_test)\n",
    "X_testdata_transformed = transformer.transform(X_test_data)\n",
    "\n",
    "clf = MLPClassifier(solver='adam', alpha=1e-2, hidden_layer_sizes=(5, 3), random_state=1)\n",
    "clf.fit(X_train, y_train)\n",
    "print(f'Training accuracy: {clf.score(X_train, y_train)}')\n",
    "print(f'Testing accuracy: {clf.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False,  True, False, ...,  True,  True, False])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(X_testdata_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['prediction'] = clf.predict(X_testdata_transformed).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_save = df_test[['project_id','prediction']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_save.to_csv('predictions.tsv', sep='\\t', columns=['project_id','prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>project_id</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KS_100001</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>KS_100003</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>KS_100004</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>KS_100005</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>KS_100006</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78057</th>\n",
       "      <td>KS_178057</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78058</th>\n",
       "      <td>KS_178058</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78061</th>\n",
       "      <td>KS_178061</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78062</th>\n",
       "      <td>KS_178062</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78063</th>\n",
       "      <td>KS_178063</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>37118 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      project_id  prediction\n",
       "1      KS_100001           1\n",
       "3      KS_100003           1\n",
       "4      KS_100004           1\n",
       "5      KS_100005           1\n",
       "6      KS_100006           1\n",
       "...          ...         ...\n",
       "78057  KS_178057           1\n",
       "78058  KS_178058           1\n",
       "78061  KS_178061           1\n",
       "78062  KS_178062           1\n",
       "78063  KS_178063           1\n",
       "\n",
       "[37118 rows x 2 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_save.loc[df_save.prediction == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rate_funded_sub',\n",
       " 'rate_funded_continent',\n",
       " 'rate_funded_cat',\n",
       " 'delta_time_launched_days',\n",
       " 'goal_converted_log',\n",
       " 'staff_pick']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>project_id</th>\n",
       "      <th>blurb</th>\n",
       "      <th>category</th>\n",
       "      <th>country</th>\n",
       "      <th>created_at</th>\n",
       "      <th>currency</th>\n",
       "      <th>deadline</th>\n",
       "      <th>fx_rate</th>\n",
       "      <th>goal</th>\n",
       "      <th>launched_at</th>\n",
       "      <th>...</th>\n",
       "      <th>rate_funded_country</th>\n",
       "      <th>rate_funded_continent</th>\n",
       "      <th>rate_funded_sub</th>\n",
       "      <th>delta_time_created</th>\n",
       "      <th>delta_time_launched</th>\n",
       "      <th>delta_time_launched_days</th>\n",
       "      <th>delta_time_created_days</th>\n",
       "      <th>goal_converted_log</th>\n",
       "      <th>goal_per_day</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KS_100000</td>\n",
       "      <td>We are looking to bring a Visiting Sculptor fr...</td>\n",
       "      <td>art</td>\n",
       "      <td>US</td>\n",
       "      <td>1330727362</td>\n",
       "      <td>USD</td>\n",
       "      <td>1334959598</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3900.0</td>\n",
       "      <td>1332972398</td>\n",
       "      <td>...</td>\n",
       "      <td>0.608791</td>\n",
       "      <td>0.602472</td>\n",
       "      <td>0.451417</td>\n",
       "      <td>8979093.0</td>\n",
       "      <td>1990338.0</td>\n",
       "      <td>23.036319</td>\n",
       "      <td>23.036319</td>\n",
       "      <td>7.937375</td>\n",
       "      <td>3.987953e-06</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KS_100001</td>\n",
       "      <td>Surrealistic oil paintings capturing the metam...</td>\n",
       "      <td>art</td>\n",
       "      <td>US</td>\n",
       "      <td>1332598567</td>\n",
       "      <td>USD</td>\n",
       "      <td>1334635140</td>\n",
       "      <td>1.0</td>\n",
       "      <td>750.0</td>\n",
       "      <td>1332975679</td>\n",
       "      <td>...</td>\n",
       "      <td>0.608791</td>\n",
       "      <td>0.602472</td>\n",
       "      <td>0.679271</td>\n",
       "      <td>3709056.0</td>\n",
       "      <td>1650995.0</td>\n",
       "      <td>19.108738</td>\n",
       "      <td>19.108738</td>\n",
       "      <td>8.411833</td>\n",
       "      <td>5.095008e-06</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>KS_100002</td>\n",
       "      <td>P.M.A.F.T.W. my upcoming solo show June 2012 a...</td>\n",
       "      <td>art</td>\n",
       "      <td>US</td>\n",
       "      <td>1332476607</td>\n",
       "      <td>USD</td>\n",
       "      <td>1333421843</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1332989843</td>\n",
       "      <td>...</td>\n",
       "      <td>0.608791</td>\n",
       "      <td>0.602472</td>\n",
       "      <td>0.481656</td>\n",
       "      <td>1038949.0</td>\n",
       "      <td>430104.0</td>\n",
       "      <td>4.978056</td>\n",
       "      <td>4.978056</td>\n",
       "      <td>8.517193</td>\n",
       "      <td>1.980264e-05</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>KS_100003</td>\n",
       "      <td>A series of images about the art/artist and lo...</td>\n",
       "      <td>art</td>\n",
       "      <td>US</td>\n",
       "      <td>1327538509</td>\n",
       "      <td>USD</td>\n",
       "      <td>1338231398</td>\n",
       "      <td>1.0</td>\n",
       "      <td>15000.0</td>\n",
       "      <td>1333047398</td>\n",
       "      <td>...</td>\n",
       "      <td>0.541360</td>\n",
       "      <td>0.494152</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5888947.0</td>\n",
       "      <td>5239702.0</td>\n",
       "      <td>60.644699</td>\n",
       "      <td>60.644699</td>\n",
       "      <td>8.779557</td>\n",
       "      <td>1.675583e-06</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>KS_100004</td>\n",
       "      <td>Become a part of the first crowd-funded commun...</td>\n",
       "      <td>art</td>\n",
       "      <td>US</td>\n",
       "      <td>1332705528</td>\n",
       "      <td>USD</td>\n",
       "      <td>1334606400</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4200.0</td>\n",
       "      <td>1333049586</td>\n",
       "      <td>...</td>\n",
       "      <td>0.608791</td>\n",
       "      <td>0.602472</td>\n",
       "      <td>0.679271</td>\n",
       "      <td>6043783.0</td>\n",
       "      <td>1614160.0</td>\n",
       "      <td>18.682407</td>\n",
       "      <td>18.682407</td>\n",
       "      <td>9.615805</td>\n",
       "      <td>5.957158e-06</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78060</th>\n",
       "      <td>KS_178060</td>\n",
       "      <td>Your opportunity to help improvMANIA open Chan...</td>\n",
       "      <td>theater</td>\n",
       "      <td>US</td>\n",
       "      <td>1405399445</td>\n",
       "      <td>USD</td>\n",
       "      <td>1410324720</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7900.0</td>\n",
       "      <td>1407784586</td>\n",
       "      <td>...</td>\n",
       "      <td>0.608791</td>\n",
       "      <td>0.602472</td>\n",
       "      <td>0.614325</td>\n",
       "      <td>7551465.0</td>\n",
       "      <td>6123249.0</td>\n",
       "      <td>70.870937</td>\n",
       "      <td>70.870937</td>\n",
       "      <td>9.975808</td>\n",
       "      <td>1.629169e-06</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78061</th>\n",
       "      <td>KS_178061</td>\n",
       "      <td>Dad's Garage Theatre Company needs your help b...</td>\n",
       "      <td>theater</td>\n",
       "      <td>US</td>\n",
       "      <td>1409848035</td>\n",
       "      <td>USD</td>\n",
       "      <td>1415722236</td>\n",
       "      <td>1.0</td>\n",
       "      <td>116000.0</td>\n",
       "      <td>1410534636</td>\n",
       "      <td>...</td>\n",
       "      <td>0.608791</td>\n",
       "      <td>0.602472</td>\n",
       "      <td>0.614325</td>\n",
       "      <td>13659097.0</td>\n",
       "      <td>11470272.0</td>\n",
       "      <td>132.757778</td>\n",
       "      <td>132.757778</td>\n",
       "      <td>8.517193</td>\n",
       "      <td>7.425450e-07</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78062</th>\n",
       "      <td>KS_178062</td>\n",
       "      <td>A new performance space in Seattle. A place fo...</td>\n",
       "      <td>theater</td>\n",
       "      <td>US</td>\n",
       "      <td>1364405045</td>\n",
       "      <td>USD</td>\n",
       "      <td>1369637940</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8000.0</td>\n",
       "      <td>1367088443</td>\n",
       "      <td>...</td>\n",
       "      <td>0.608791</td>\n",
       "      <td>0.602472</td>\n",
       "      <td>0.614325</td>\n",
       "      <td>-35230560.0</td>\n",
       "      <td>-35326380.0</td>\n",
       "      <td>-408.870139</td>\n",
       "      <td>-408.870139</td>\n",
       "      <td>10.819778</td>\n",
       "      <td>-3.062804e-07</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78063</th>\n",
       "      <td>KS_178063</td>\n",
       "      <td>After 22 yrs downstairs we are \"getting out of...</td>\n",
       "      <td>theater</td>\n",
       "      <td>US</td>\n",
       "      <td>1385067433</td>\n",
       "      <td>USD</td>\n",
       "      <td>1388303940</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>1386011038</td>\n",
       "      <td>...</td>\n",
       "      <td>0.608791</td>\n",
       "      <td>0.602472</td>\n",
       "      <td>0.614325</td>\n",
       "      <td>-16549566.0</td>\n",
       "      <td>-17138690.0</td>\n",
       "      <td>-198.364468</td>\n",
       "      <td>-198.364468</td>\n",
       "      <td>9.615805</td>\n",
       "      <td>-5.610584e-07</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78064</th>\n",
       "      <td>KS_178064</td>\n",
       "      <td>We plan to transition from 35mm to the new dig...</td>\n",
       "      <td>theater</td>\n",
       "      <td>US</td>\n",
       "      <td>1398433509</td>\n",
       "      <td>USD</td>\n",
       "      <td>1401159600</td>\n",
       "      <td>1.0</td>\n",
       "      <td>15000.0</td>\n",
       "      <td>1398801620</td>\n",
       "      <td>...</td>\n",
       "      <td>0.608791</td>\n",
       "      <td>0.602472</td>\n",
       "      <td>0.614325</td>\n",
       "      <td>-3675240.0</td>\n",
       "      <td>-4314840.0</td>\n",
       "      <td>-49.940278</td>\n",
       "      <td>-49.940278</td>\n",
       "      <td>10.714418</td>\n",
       "      <td>-2.483155e-06</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>78065 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      project_id                                              blurb category  \\\n",
       "0      KS_100000  We are looking to bring a Visiting Sculptor fr...      art   \n",
       "1      KS_100001  Surrealistic oil paintings capturing the metam...      art   \n",
       "2      KS_100002  P.M.A.F.T.W. my upcoming solo show June 2012 a...      art   \n",
       "3      KS_100003  A series of images about the art/artist and lo...      art   \n",
       "4      KS_100004  Become a part of the first crowd-funded commun...      art   \n",
       "...          ...                                                ...      ...   \n",
       "78060  KS_178060  Your opportunity to help improvMANIA open Chan...  theater   \n",
       "78061  KS_178061  Dad's Garage Theatre Company needs your help b...  theater   \n",
       "78062  KS_178062  A new performance space in Seattle. A place fo...  theater   \n",
       "78063  KS_178063  After 22 yrs downstairs we are \"getting out of...  theater   \n",
       "78064  KS_178064  We plan to transition from 35mm to the new dig...  theater   \n",
       "\n",
       "      country  created_at currency    deadline  fx_rate      goal  \\\n",
       "0          US  1330727362      USD  1334959598      1.0    3900.0   \n",
       "1          US  1332598567      USD  1334635140      1.0     750.0   \n",
       "2          US  1332476607      USD  1333421843      1.0    1000.0   \n",
       "3          US  1327538509      USD  1338231398      1.0   15000.0   \n",
       "4          US  1332705528      USD  1334606400      1.0    4200.0   \n",
       "...       ...         ...      ...         ...      ...       ...   \n",
       "78060      US  1405399445      USD  1410324720      1.0    7900.0   \n",
       "78061      US  1409848035      USD  1415722236      1.0  116000.0   \n",
       "78062      US  1364405045      USD  1369637940      1.0    8000.0   \n",
       "78063      US  1385067433      USD  1388303940      1.0   20000.0   \n",
       "78064      US  1398433509      USD  1401159600      1.0   15000.0   \n",
       "\n",
       "       launched_at  ... rate_funded_country  rate_funded_continent  \\\n",
       "0       1332972398  ...            0.608791               0.602472   \n",
       "1       1332975679  ...            0.608791               0.602472   \n",
       "2       1332989843  ...            0.608791               0.602472   \n",
       "3       1333047398  ...            0.541360               0.494152   \n",
       "4       1333049586  ...            0.608791               0.602472   \n",
       "...            ...  ...                 ...                    ...   \n",
       "78060   1407784586  ...            0.608791               0.602472   \n",
       "78061   1410534636  ...            0.608791               0.602472   \n",
       "78062   1367088443  ...            0.608791               0.602472   \n",
       "78063   1386011038  ...            0.608791               0.602472   \n",
       "78064   1398801620  ...            0.608791               0.602472   \n",
       "\n",
       "      rate_funded_sub delta_time_created delta_time_launched  \\\n",
       "0            0.451417          8979093.0           1990338.0   \n",
       "1            0.679271          3709056.0           1650995.0   \n",
       "2            0.481656          1038949.0            430104.0   \n",
       "3            1.000000          5888947.0           5239702.0   \n",
       "4            0.679271          6043783.0           1614160.0   \n",
       "...               ...                ...                 ...   \n",
       "78060        0.614325          7551465.0           6123249.0   \n",
       "78061        0.614325         13659097.0          11470272.0   \n",
       "78062        0.614325        -35230560.0         -35326380.0   \n",
       "78063        0.614325        -16549566.0         -17138690.0   \n",
       "78064        0.614325         -3675240.0          -4314840.0   \n",
       "\n",
       "      delta_time_launched_days delta_time_created_days  goal_converted_log  \\\n",
       "0                    23.036319               23.036319            7.937375   \n",
       "1                    19.108738               19.108738            8.411833   \n",
       "2                     4.978056                4.978056            8.517193   \n",
       "3                    60.644699               60.644699            8.779557   \n",
       "4                    18.682407               18.682407            9.615805   \n",
       "...                        ...                     ...                 ...   \n",
       "78060                70.870937               70.870937            9.975808   \n",
       "78061               132.757778              132.757778            8.517193   \n",
       "78062              -408.870139             -408.870139           10.819778   \n",
       "78063              -198.364468             -198.364468            9.615805   \n",
       "78064               -49.940278              -49.940278           10.714418   \n",
       "\n",
       "       goal_per_day  prediction  \n",
       "0      3.987953e-06           0  \n",
       "1      5.095008e-06           1  \n",
       "2      1.980264e-05           0  \n",
       "3      1.675583e-06           1  \n",
       "4      5.957158e-06           1  \n",
       "...             ...         ...  \n",
       "78060  1.629169e-06           0  \n",
       "78061  7.425450e-07           1  \n",
       "78062 -3.062804e-07           1  \n",
       "78063 -5.610584e-07           1  \n",
       "78064 -2.483155e-06           0  \n",
       "\n",
       "[78065 rows x 28 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61671.350000000006"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "78065 * .79"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import SCORERS\n",
    "\n",
    "mlp = make_pipeline(RobustScaler(), MLPClassifier(solver='adam', alpha=1e-2, hidden_layer_sizes=(5, 3), random_state=1))\n",
    "scoring = ['accuracy', 'f1', 'precision', 'recall']\n",
    "scores = cross_validate(mlp, X, y, scoring=scoring, cv=5, return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: 0.82 (+/- 0.02)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([10.92324281,  7.57475805,  9.05897975,  7.21579838,  7.59323978]),\n",
       " 'score_time': array([0.02299905, 0.02318907, 0.02277327, 0.0227499 , 0.02298975]),\n",
       " 'test_accuracy': array([0.74455, 0.78735, 0.81495, 0.79055, 0.80405]),\n",
       " 'train_accuracy': array([0.8042625, 0.7929   , 0.787    , 0.791675 , 0.788    ]),\n",
       " 'test_f1': array([0.79960777, 0.80956432, 0.8248379 , 0.82396941, 0.82097666]),\n",
       " 'train_f1': array([0.82637572, 0.81854821, 0.81398598, 0.82009543, 0.81341313]),\n",
       " 'test_precision': array([0.73840916, 0.84946439, 0.92328883, 0.80984636, 0.88089403]),\n",
       " 'train_precision': array([0.85812647, 0.83892693, 0.83141181, 0.8280327 , 0.83766881]),\n",
       " 'test_recall': array([0.87186725, 0.77324438, 0.74535968, 0.83859379, 0.76869119]),\n",
       " 'train_recall': array([0.7968907 , 0.79913607, 0.79727562, 0.81230888, 0.79052262])}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"f1: %0.2f (+/- %0.2f)\" % (scores['test_f1'].mean(), scores['test_f1'].std() * 2))\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tuning hyper-parameters for accuracy\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'alpha': 1e-07, 'hidden_layer_sizes': (32, 8), 'solver': 'adam'}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.788 (+/-0.005) for {'alpha': 0.1, 'hidden_layer_sizes': (4, 4), 'solver': 'adam'}\n",
      "0.789 (+/-0.004) for {'alpha': 0.1, 'hidden_layer_sizes': (5, 5), 'solver': 'adam'}\n",
      "0.790 (+/-0.008) for {'alpha': 0.1, 'hidden_layer_sizes': (6, 6), 'solver': 'adam'}\n",
      "0.790 (+/-0.007) for {'alpha': 0.1, 'hidden_layer_sizes': (8, 5), 'solver': 'adam'}\n",
      "0.791 (+/-0.005) for {'alpha': 0.1, 'hidden_layer_sizes': (8, 8), 'solver': 'adam'}\n",
      "0.792 (+/-0.005) for {'alpha': 0.1, 'hidden_layer_sizes': (16, 4), 'solver': 'adam'}\n",
      "0.793 (+/-0.003) for {'alpha': 0.1, 'hidden_layer_sizes': (16, 8), 'solver': 'adam'}\n",
      "0.792 (+/-0.004) for {'alpha': 0.1, 'hidden_layer_sizes': (16, 16), 'solver': 'adam'}\n",
      "0.792 (+/-0.003) for {'alpha': 0.1, 'hidden_layer_sizes': (32, 8), 'solver': 'adam'}\n",
      "0.788 (+/-0.005) for {'alpha': 0.001, 'hidden_layer_sizes': (4, 4), 'solver': 'adam'}\n",
      "0.787 (+/-0.005) for {'alpha': 0.001, 'hidden_layer_sizes': (5, 5), 'solver': 'adam'}\n",
      "0.791 (+/-0.005) for {'alpha': 0.001, 'hidden_layer_sizes': (6, 6), 'solver': 'adam'}\n",
      "0.792 (+/-0.004) for {'alpha': 0.001, 'hidden_layer_sizes': (8, 5), 'solver': 'adam'}\n",
      "0.792 (+/-0.004) for {'alpha': 0.001, 'hidden_layer_sizes': (8, 8), 'solver': 'adam'}\n",
      "0.792 (+/-0.004) for {'alpha': 0.001, 'hidden_layer_sizes': (16, 4), 'solver': 'adam'}\n",
      "0.793 (+/-0.003) for {'alpha': 0.001, 'hidden_layer_sizes': (16, 8), 'solver': 'adam'}\n",
      "0.793 (+/-0.006) for {'alpha': 0.001, 'hidden_layer_sizes': (16, 16), 'solver': 'adam'}\n",
      "0.793 (+/-0.004) for {'alpha': 0.001, 'hidden_layer_sizes': (32, 8), 'solver': 'adam'}\n",
      "0.788 (+/-0.005) for {'alpha': 1e-05, 'hidden_layer_sizes': (4, 4), 'solver': 'adam'}\n",
      "0.788 (+/-0.004) for {'alpha': 1e-05, 'hidden_layer_sizes': (5, 5), 'solver': 'adam'}\n",
      "0.789 (+/-0.007) for {'alpha': 1e-05, 'hidden_layer_sizes': (6, 6), 'solver': 'adam'}\n",
      "0.791 (+/-0.006) for {'alpha': 1e-05, 'hidden_layer_sizes': (8, 5), 'solver': 'adam'}\n",
      "0.792 (+/-0.003) for {'alpha': 1e-05, 'hidden_layer_sizes': (8, 8), 'solver': 'adam'}\n",
      "0.793 (+/-0.003) for {'alpha': 1e-05, 'hidden_layer_sizes': (16, 4), 'solver': 'adam'}\n",
      "0.794 (+/-0.003) for {'alpha': 1e-05, 'hidden_layer_sizes': (16, 8), 'solver': 'adam'}\n",
      "0.793 (+/-0.006) for {'alpha': 1e-05, 'hidden_layer_sizes': (16, 16), 'solver': 'adam'}\n",
      "0.793 (+/-0.004) for {'alpha': 1e-05, 'hidden_layer_sizes': (32, 8), 'solver': 'adam'}\n",
      "0.788 (+/-0.007) for {'alpha': 1e-07, 'hidden_layer_sizes': (4, 4), 'solver': 'adam'}\n",
      "0.790 (+/-0.006) for {'alpha': 1e-07, 'hidden_layer_sizes': (5, 5), 'solver': 'adam'}\n",
      "0.790 (+/-0.005) for {'alpha': 1e-07, 'hidden_layer_sizes': (6, 6), 'solver': 'adam'}\n",
      "0.792 (+/-0.005) for {'alpha': 1e-07, 'hidden_layer_sizes': (8, 5), 'solver': 'adam'}\n",
      "0.792 (+/-0.006) for {'alpha': 1e-07, 'hidden_layer_sizes': (8, 8), 'solver': 'adam'}\n",
      "0.793 (+/-0.004) for {'alpha': 1e-07, 'hidden_layer_sizes': (16, 4), 'solver': 'adam'}\n",
      "0.794 (+/-0.004) for {'alpha': 1e-07, 'hidden_layer_sizes': (16, 8), 'solver': 'adam'}\n",
      "0.793 (+/-0.005) for {'alpha': 1e-07, 'hidden_layer_sizes': (16, 16), 'solver': 'adam'}\n",
      "0.795 (+/-0.003) for {'alpha': 1e-07, 'hidden_layer_sizes': (32, 8), 'solver': 'adam'}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.75      0.76      0.76     12519\n",
      "        True       0.83      0.82      0.82     17481\n",
      "\n",
      "    accuracy                           0.79     30000\n",
      "   macro avg       0.79      0.79      0.79     30000\n",
      "weighted avg       0.79      0.79      0.79     30000\n",
      "\n",
      "\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'alpha': 1e-05, 'hidden_layer_sizes': (16, 16), 'solver': 'adam'}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.814 (+/-0.004) for {'alpha': 0.1, 'hidden_layer_sizes': (4, 4), 'solver': 'adam'}\n",
      "0.815 (+/-0.004) for {'alpha': 0.1, 'hidden_layer_sizes': (5, 5), 'solver': 'adam'}\n",
      "0.815 (+/-0.003) for {'alpha': 0.1, 'hidden_layer_sizes': (6, 6), 'solver': 'adam'}\n",
      "0.817 (+/-0.003) for {'alpha': 0.1, 'hidden_layer_sizes': (8, 5), 'solver': 'adam'}\n",
      "0.819 (+/-0.005) for {'alpha': 0.1, 'hidden_layer_sizes': (8, 8), 'solver': 'adam'}\n",
      "0.817 (+/-0.005) for {'alpha': 0.1, 'hidden_layer_sizes': (16, 4), 'solver': 'adam'}\n",
      "0.818 (+/-0.004) for {'alpha': 0.1, 'hidden_layer_sizes': (16, 8), 'solver': 'adam'}\n",
      "0.819 (+/-0.005) for {'alpha': 0.1, 'hidden_layer_sizes': (16, 16), 'solver': 'adam'}\n",
      "0.819 (+/-0.006) for {'alpha': 0.1, 'hidden_layer_sizes': (32, 8), 'solver': 'adam'}\n",
      "0.814 (+/-0.006) for {'alpha': 0.001, 'hidden_layer_sizes': (4, 4), 'solver': 'adam'}\n",
      "0.816 (+/-0.006) for {'alpha': 0.001, 'hidden_layer_sizes': (5, 5), 'solver': 'adam'}\n",
      "0.816 (+/-0.005) for {'alpha': 0.001, 'hidden_layer_sizes': (6, 6), 'solver': 'adam'}\n",
      "0.818 (+/-0.004) for {'alpha': 0.001, 'hidden_layer_sizes': (8, 5), 'solver': 'adam'}\n",
      "0.819 (+/-0.004) for {'alpha': 0.001, 'hidden_layer_sizes': (8, 8), 'solver': 'adam'}\n",
      "0.820 (+/-0.003) for {'alpha': 0.001, 'hidden_layer_sizes': (16, 4), 'solver': 'adam'}\n",
      "0.820 (+/-0.003) for {'alpha': 0.001, 'hidden_layer_sizes': (16, 8), 'solver': 'adam'}\n",
      "0.819 (+/-0.008) for {'alpha': 0.001, 'hidden_layer_sizes': (16, 16), 'solver': 'adam'}\n",
      "0.821 (+/-0.006) for {'alpha': 0.001, 'hidden_layer_sizes': (32, 8), 'solver': 'adam'}\n",
      "0.814 (+/-0.008) for {'alpha': 1e-05, 'hidden_layer_sizes': (4, 4), 'solver': 'adam'}\n",
      "0.816 (+/-0.009) for {'alpha': 1e-05, 'hidden_layer_sizes': (5, 5), 'solver': 'adam'}\n",
      "0.817 (+/-0.006) for {'alpha': 1e-05, 'hidden_layer_sizes': (6, 6), 'solver': 'adam'}\n",
      "0.817 (+/-0.005) for {'alpha': 1e-05, 'hidden_layer_sizes': (8, 5), 'solver': 'adam'}\n",
      "0.818 (+/-0.004) for {'alpha': 1e-05, 'hidden_layer_sizes': (8, 8), 'solver': 'adam'}\n",
      "0.819 (+/-0.005) for {'alpha': 1e-05, 'hidden_layer_sizes': (16, 4), 'solver': 'adam'}\n",
      "0.821 (+/-0.006) for {'alpha': 1e-05, 'hidden_layer_sizes': (16, 8), 'solver': 'adam'}\n",
      "0.822 (+/-0.003) for {'alpha': 1e-05, 'hidden_layer_sizes': (16, 16), 'solver': 'adam'}\n",
      "0.821 (+/-0.004) for {'alpha': 1e-05, 'hidden_layer_sizes': (32, 8), 'solver': 'adam'}\n",
      "0.815 (+/-0.003) for {'alpha': 1e-07, 'hidden_layer_sizes': (4, 4), 'solver': 'adam'}\n",
      "0.816 (+/-0.003) for {'alpha': 1e-07, 'hidden_layer_sizes': (5, 5), 'solver': 'adam'}\n",
      "0.815 (+/-0.006) for {'alpha': 1e-07, 'hidden_layer_sizes': (6, 6), 'solver': 'adam'}\n",
      "0.819 (+/-0.004) for {'alpha': 1e-07, 'hidden_layer_sizes': (8, 5), 'solver': 'adam'}\n",
      "0.819 (+/-0.005) for {'alpha': 1e-07, 'hidden_layer_sizes': (8, 8), 'solver': 'adam'}\n",
      "0.819 (+/-0.005) for {'alpha': 1e-07, 'hidden_layer_sizes': (16, 4), 'solver': 'adam'}\n",
      "0.820 (+/-0.006) for {'alpha': 1e-07, 'hidden_layer_sizes': (16, 8), 'solver': 'adam'}\n",
      "0.820 (+/-0.002) for {'alpha': 1e-07, 'hidden_layer_sizes': (16, 16), 'solver': 'adam'}\n",
      "0.821 (+/-0.007) for {'alpha': 1e-07, 'hidden_layer_sizes': (32, 8), 'solver': 'adam'}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.74      0.79      0.76     12519\n",
      "        True       0.84      0.80      0.82     17481\n",
      "\n",
      "    accuracy                           0.79     30000\n",
      "   macro avg       0.79      0.79      0.79     30000\n",
      "weighted avg       0.80      0.79      0.80     30000\n",
      "\n",
      "\n",
      "# Tuning hyper-parameters for precision\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'alpha': 0.001, 'hidden_layer_sizes': (16, 16), 'solver': 'adam'}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.834 (+/-0.010) for {'alpha': 0.1, 'hidden_layer_sizes': (4, 4), 'solver': 'adam'}\n",
      "0.787 (+/-0.202) for {'alpha': 0.1, 'hidden_layer_sizes': (5, 5), 'solver': 'adam'}\n",
      "0.838 (+/-0.014) for {'alpha': 0.1, 'hidden_layer_sizes': (6, 6), 'solver': 'adam'}\n",
      "0.841 (+/-0.008) for {'alpha': 0.1, 'hidden_layer_sizes': (8, 5), 'solver': 'adam'}\n",
      "0.836 (+/-0.008) for {'alpha': 0.1, 'hidden_layer_sizes': (8, 8), 'solver': 'adam'}\n",
      "0.840 (+/-0.013) for {'alpha': 0.1, 'hidden_layer_sizes': (16, 4), 'solver': 'adam'}\n",
      "0.839 (+/-0.009) for {'alpha': 0.1, 'hidden_layer_sizes': (16, 8), 'solver': 'adam'}\n",
      "0.841 (+/-0.019) for {'alpha': 0.1, 'hidden_layer_sizes': (16, 16), 'solver': 'adam'}\n",
      "0.840 (+/-0.011) for {'alpha': 0.1, 'hidden_layer_sizes': (32, 8), 'solver': 'adam'}\n",
      "0.833 (+/-0.016) for {'alpha': 0.001, 'hidden_layer_sizes': (4, 4), 'solver': 'adam'}\n",
      "0.833 (+/-0.011) for {'alpha': 0.001, 'hidden_layer_sizes': (5, 5), 'solver': 'adam'}\n",
      "0.837 (+/-0.012) for {'alpha': 0.001, 'hidden_layer_sizes': (6, 6), 'solver': 'adam'}\n",
      "0.836 (+/-0.006) for {'alpha': 0.001, 'hidden_layer_sizes': (8, 5), 'solver': 'adam'}\n",
      "0.837 (+/-0.014) for {'alpha': 0.001, 'hidden_layer_sizes': (8, 8), 'solver': 'adam'}\n",
      "0.836 (+/-0.013) for {'alpha': 0.001, 'hidden_layer_sizes': (16, 4), 'solver': 'adam'}\n",
      "0.835 (+/-0.006) for {'alpha': 0.001, 'hidden_layer_sizes': (16, 8), 'solver': 'adam'}\n",
      "0.844 (+/-0.016) for {'alpha': 0.001, 'hidden_layer_sizes': (16, 16), 'solver': 'adam'}\n",
      "0.835 (+/-0.013) for {'alpha': 0.001, 'hidden_layer_sizes': (32, 8), 'solver': 'adam'}\n",
      "0.837 (+/-0.012) for {'alpha': 1e-05, 'hidden_layer_sizes': (4, 4), 'solver': 'adam'}\n",
      "0.833 (+/-0.016) for {'alpha': 1e-05, 'hidden_layer_sizes': (5, 5), 'solver': 'adam'}\n",
      "0.830 (+/-0.016) for {'alpha': 1e-05, 'hidden_layer_sizes': (6, 6), 'solver': 'adam'}\n",
      "0.839 (+/-0.012) for {'alpha': 1e-05, 'hidden_layer_sizes': (8, 5), 'solver': 'adam'}\n",
      "0.831 (+/-0.010) for {'alpha': 1e-05, 'hidden_layer_sizes': (8, 8), 'solver': 'adam'}\n",
      "0.839 (+/-0.013) for {'alpha': 1e-05, 'hidden_layer_sizes': (16, 4), 'solver': 'adam'}\n",
      "0.833 (+/-0.009) for {'alpha': 1e-05, 'hidden_layer_sizes': (16, 8), 'solver': 'adam'}\n",
      "0.831 (+/-0.016) for {'alpha': 1e-05, 'hidden_layer_sizes': (16, 16), 'solver': 'adam'}\n",
      "0.840 (+/-0.009) for {'alpha': 1e-05, 'hidden_layer_sizes': (32, 8), 'solver': 'adam'}\n",
      "0.832 (+/-0.008) for {'alpha': 1e-07, 'hidden_layer_sizes': (4, 4), 'solver': 'adam'}\n",
      "0.838 (+/-0.012) for {'alpha': 1e-07, 'hidden_layer_sizes': (5, 5), 'solver': 'adam'}\n",
      "0.833 (+/-0.020) for {'alpha': 1e-07, 'hidden_layer_sizes': (6, 6), 'solver': 'adam'}\n",
      "0.834 (+/-0.008) for {'alpha': 1e-07, 'hidden_layer_sizes': (8, 5), 'solver': 'adam'}\n",
      "0.834 (+/-0.005) for {'alpha': 1e-07, 'hidden_layer_sizes': (8, 8), 'solver': 'adam'}\n",
      "0.842 (+/-0.021) for {'alpha': 1e-07, 'hidden_layer_sizes': (16, 4), 'solver': 'adam'}\n",
      "0.843 (+/-0.008) for {'alpha': 1e-07, 'hidden_layer_sizes': (16, 8), 'solver': 'adam'}\n",
      "0.842 (+/-0.016) for {'alpha': 1e-07, 'hidden_layer_sizes': (16, 16), 'solver': 'adam'}\n",
      "0.840 (+/-0.018) for {'alpha': 1e-07, 'hidden_layer_sizes': (32, 8), 'solver': 'adam'}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.73      0.80      0.77     12519\n",
      "        True       0.85      0.79      0.82     17481\n",
      "\n",
      "    accuracy                           0.79     30000\n",
      "   macro avg       0.79      0.80      0.79     30000\n",
      "weighted avg       0.80      0.79      0.80     30000\n",
      "\n",
      "\n",
      "# Tuning hyper-parameters for recall\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on development set:\n",
      "\n",
      "{'alpha': 1e-05, 'hidden_layer_sizes': (32, 8), 'solver': 'adam'}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.799 (+/-0.012) for {'alpha': 0.1, 'hidden_layer_sizes': (4, 4), 'solver': 'adam'}\n",
      "0.797 (+/-0.018) for {'alpha': 0.1, 'hidden_layer_sizes': (5, 5), 'solver': 'adam'}\n",
      "0.798 (+/-0.018) for {'alpha': 0.1, 'hidden_layer_sizes': (6, 6), 'solver': 'adam'}\n",
      "0.800 (+/-0.022) for {'alpha': 0.1, 'hidden_layer_sizes': (8, 5), 'solver': 'adam'}\n",
      "0.806 (+/-0.012) for {'alpha': 0.1, 'hidden_layer_sizes': (8, 8), 'solver': 'adam'}\n",
      "0.804 (+/-0.013) for {'alpha': 0.1, 'hidden_layer_sizes': (16, 4), 'solver': 'adam'}\n",
      "0.792 (+/-0.027) for {'alpha': 0.1, 'hidden_layer_sizes': (16, 8), 'solver': 'adam'}\n",
      "0.801 (+/-0.015) for {'alpha': 0.1, 'hidden_layer_sizes': (16, 16), 'solver': 'adam'}\n",
      "0.793 (+/-0.008) for {'alpha': 0.1, 'hidden_layer_sizes': (32, 8), 'solver': 'adam'}\n",
      "0.801 (+/-0.017) for {'alpha': 0.001, 'hidden_layer_sizes': (4, 4), 'solver': 'adam'}\n",
      "0.798 (+/-0.023) for {'alpha': 0.001, 'hidden_layer_sizes': (5, 5), 'solver': 'adam'}\n",
      "0.791 (+/-0.014) for {'alpha': 0.001, 'hidden_layer_sizes': (6, 6), 'solver': 'adam'}\n",
      "0.802 (+/-0.022) for {'alpha': 0.001, 'hidden_layer_sizes': (8, 5), 'solver': 'adam'}\n",
      "0.799 (+/-0.009) for {'alpha': 0.001, 'hidden_layer_sizes': (8, 8), 'solver': 'adam'}\n",
      "0.802 (+/-0.016) for {'alpha': 0.001, 'hidden_layer_sizes': (16, 4), 'solver': 'adam'}\n",
      "0.803 (+/-0.020) for {'alpha': 0.001, 'hidden_layer_sizes': (16, 8), 'solver': 'adam'}\n",
      "0.807 (+/-0.022) for {'alpha': 0.001, 'hidden_layer_sizes': (16, 16), 'solver': 'adam'}\n",
      "0.807 (+/-0.021) for {'alpha': 0.001, 'hidden_layer_sizes': (32, 8), 'solver': 'adam'}\n",
      "0.795 (+/-0.016) for {'alpha': 1e-05, 'hidden_layer_sizes': (4, 4), 'solver': 'adam'}\n",
      "0.799 (+/-0.004) for {'alpha': 1e-05, 'hidden_layer_sizes': (5, 5), 'solver': 'adam'}\n",
      "0.792 (+/-0.026) for {'alpha': 1e-05, 'hidden_layer_sizes': (6, 6), 'solver': 'adam'}\n",
      "0.795 (+/-0.010) for {'alpha': 1e-05, 'hidden_layer_sizes': (8, 5), 'solver': 'adam'}\n",
      "0.798 (+/-0.018) for {'alpha': 1e-05, 'hidden_layer_sizes': (8, 8), 'solver': 'adam'}\n",
      "0.797 (+/-0.011) for {'alpha': 1e-05, 'hidden_layer_sizes': (16, 4), 'solver': 'adam'}\n",
      "0.806 (+/-0.011) for {'alpha': 1e-05, 'hidden_layer_sizes': (16, 8), 'solver': 'adam'}\n",
      "0.804 (+/-0.024) for {'alpha': 1e-05, 'hidden_layer_sizes': (16, 16), 'solver': 'adam'}\n",
      "0.811 (+/-0.017) for {'alpha': 1e-05, 'hidden_layer_sizes': (32, 8), 'solver': 'adam'}\n",
      "0.791 (+/-0.019) for {'alpha': 1e-07, 'hidden_layer_sizes': (4, 4), 'solver': 'adam'}\n",
      "0.799 (+/-0.013) for {'alpha': 1e-07, 'hidden_layer_sizes': (5, 5), 'solver': 'adam'}\n",
      "0.789 (+/-0.010) for {'alpha': 1e-07, 'hidden_layer_sizes': (6, 6), 'solver': 'adam'}\n",
      "0.796 (+/-0.020) for {'alpha': 1e-07, 'hidden_layer_sizes': (8, 5), 'solver': 'adam'}\n",
      "0.803 (+/-0.006) for {'alpha': 1e-07, 'hidden_layer_sizes': (8, 8), 'solver': 'adam'}\n",
      "0.798 (+/-0.015) for {'alpha': 1e-07, 'hidden_layer_sizes': (16, 4), 'solver': 'adam'}\n",
      "0.792 (+/-0.019) for {'alpha': 1e-07, 'hidden_layer_sizes': (16, 8), 'solver': 'adam'}\n",
      "0.809 (+/-0.021) for {'alpha': 1e-07, 'hidden_layer_sizes': (16, 16), 'solver': 'adam'}\n",
      "0.801 (+/-0.027) for {'alpha': 1e-07, 'hidden_layer_sizes': (32, 8), 'solver': 'adam'}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.73      0.80      0.77     12519\n",
      "        True       0.85      0.79      0.82     17481\n",
      "\n",
      "    accuracy                           0.80     30000\n",
      "   macro avg       0.79      0.80      0.79     30000\n",
      "weighted avg       0.80      0.80      0.80     30000\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import SCORERS\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X, y, test_size=0.3)\n",
    "transformer = RobustScaler().fit(X_train)\n",
    "X_train = transformer.transform(X_train)\n",
    "X_test = transformer.transform(X_test)\n",
    "\n",
    "tuned_parameters = [{'solver': ['adam'], 'alpha': [1e-1, 1e-3, 1e-5, 1e-7],\n",
    "                     'hidden_layer_sizes': [(4,4), (5,5), (6,6), (8, 5), (8,8), (16, 4), (16,8), (16, 16), (32, 8)]}]\n",
    "\n",
    "scores = ['accuracy', 'f1', 'precision', 'recall']\n",
    "\n",
    "for score in scores:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "\n",
    "    clf = GridSearchCV(\n",
    "        MLPClassifier(), tuned_parameters, scoring=score, return_train_score=True)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    y_true, y_pred = y_test, clf.predict(X_test)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['accuracy',\n",
       " 'adjusted_mutual_info_score',\n",
       " 'adjusted_rand_score',\n",
       " 'average_precision',\n",
       " 'balanced_accuracy',\n",
       " 'completeness_score',\n",
       " 'explained_variance',\n",
       " 'f1',\n",
       " 'f1_macro',\n",
       " 'f1_micro',\n",
       " 'f1_samples',\n",
       " 'f1_weighted',\n",
       " 'fowlkes_mallows_score',\n",
       " 'homogeneity_score',\n",
       " 'jaccard',\n",
       " 'jaccard_macro',\n",
       " 'jaccard_micro',\n",
       " 'jaccard_samples',\n",
       " 'jaccard_weighted',\n",
       " 'max_error',\n",
       " 'mutual_info_score',\n",
       " 'neg_brier_score',\n",
       " 'neg_log_loss',\n",
       " 'neg_mean_absolute_error',\n",
       " 'neg_mean_gamma_deviance',\n",
       " 'neg_mean_poisson_deviance',\n",
       " 'neg_mean_squared_error',\n",
       " 'neg_mean_squared_log_error',\n",
       " 'neg_median_absolute_error',\n",
       " 'neg_root_mean_squared_error',\n",
       " 'normalized_mutual_info_score',\n",
       " 'precision',\n",
       " 'precision_macro',\n",
       " 'precision_micro',\n",
       " 'precision_samples',\n",
       " 'precision_weighted',\n",
       " 'r2',\n",
       " 'recall',\n",
       " 'recall_macro',\n",
       " 'recall_micro',\n",
       " 'recall_samples',\n",
       " 'recall_weighted',\n",
       " 'roc_auc',\n",
       " 'roc_auc_ovo',\n",
       " 'roc_auc_ovo_weighted',\n",
       " 'roc_auc_ovr',\n",
       " 'roc_auc_ovr_weighted',\n",
       " 'v_measure_score']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(SCORERS.keys()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
