{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "Names: ['Alex Hakvoort', 'Sebastiaan van Dijk'] \n",
    "Studentnumbers: ['12488674', '12400319']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "89914f38bf3510543caaef66d0f14169",
     "grade": false,
     "grade_id": "cell-700f4a41782fa412",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Applied Machine learning\n",
    "## Practical Assignment 2\n",
    "\n",
    "### Important Notes:\n",
    "1. Submit through **Canvas** before 11:59pm on Wednesday, May 6, 2020\n",
    "2. No late homework will be accepted\n",
    "3. This is a group-of-two assignment\n",
    "4. The submitted file should be in ipynb format\n",
    "5. The assignment is worth it 10 points\n",
    "6. For questions, please use the discussion part of canvas (English only!)\n",
    "7. The indication **optional** means that the question is optional; you won't lose any points if you do not do that part of the assignment, nor will you gain if you do it.\n",
    "\n",
    "### Software:\n",
    "We will be using Python programming language throughout this course. Further we will be using:\n",
    "+ IPython Notebooks (as an environment)\n",
    "+ Numpy\n",
    "+ Pandas\n",
    "+ Scikit-learn\n",
    "\n",
    "### Background:\n",
    "\n",
    "This practical assignment will be covering logistic regression, neural networks, support vector machines and evaluation of classifiers. \n",
    "\n",
    "For the assignment, please download a dataset on Load Defaults. You are provided with two datasets:\n",
    "1. [Dataset](https://drive.google.com/open?id=1cj-CzkY6QZUe42ky64GI5CSSg7-K40N5) with 10,000 instances \n",
    "2. [Dataset](https://drive.google.com/open?id=1MbWGXLawE3VTxP1XgNpj8uEo1VHPq12B) with 100,000 instances\n",
    "In principle you should work on the second, larger dataset, but if you face scaling computational issues then better work with the first, smaller dataset.\n",
    "\n",
    "This data corresponds to a set of financial transactions associated with individuals. The data has been standardized, de-trended, and anonymized. You are provided with thousands of observations and nearly 800 features. Each observation (instance) is independent from the previous. \n",
    "\n",
    "For each observation, it was recorded whether a default was triggered. In case of a default, the loss was measured. This quantity lies between 0 and 100. It has been normalised, considering that the notional of each transaction at inception is 100. For example, a loss of 60 means that only 40 is reimbursed. If the loan did not default, the loss was 0. You are asked to predict the losses for each observation in the test set.\n",
    "\n",
    "Missing feature values have been kept as is, so that the competing teams can really use the maximum data available, implementing a strategy to fill the gaps if desired. Consider all variables continuous, even though some variables may be categorical (e.g. f776 and f777).\n",
    "\n",
    "The goal of the machine learning algorithm will be to predict whether a loan will default, given a set of features. For privacy reasons the feature names are not provided.\n",
    "\n",
    "**Important Note**: This second assignment is not as instructive as the first assignment. The first assignment guided you step-by-step through all the preprocessing, training-validation-testing setup, etc. This assignment does not do so, but it leaves it up to you to decide how to use the data and design your experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "86f687db575cf409d54ac5e91e6dd861",
     "grade": false,
     "grade_id": "cell-1979a89473cf7ece",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Part 1: Import the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "cdc9100b2eb32f795318f88531439408",
     "grade": false,
     "grade_id": "cell-3a36fe2e430fa9ab",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv('loan_default_10K.csv', sep=\",\", header=0, dtype=np.float64)\n",
    "\n",
    "# Drop the observations that contain missing values\n",
    "dfn = df.dropna(0, how='any')\n",
    "\n",
    "# Consider only a handful of features to start with; you can extend to the full set later on.\n",
    "X = dfn.loc[:,'f1':'f100'].values\n",
    "\n",
    "# Generate the labels; if 'loss' is zero the this indicates the negative class, class 0, i.e. no default;\n",
    "# if 'loss' is possitive this indicates the positive class, class 1, i.e. there is a loan default;\n",
    "y = [ bool(y) for y in dfn.loc[:,'loss'].values ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "ff730342a91fefa515c3117b502aa292",
     "grade": false,
     "grade_id": "cell-735be096b0f642ef",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Part 2: Evaluation measures (Lecture 4) (2pts)\n",
    "In what follows you should implement a number of evaluation measures. You need to implement these from scratch, meaning that it is not allowed to call any scikit-learn function, or any other API function that implements the method for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "cdadfd696f9d159f0242cad28b7f41ef",
     "grade": false,
     "grade_id": "cell-a127e8413f617d64",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "* Implement a function that produces the contigency matrix, i.e. True Positives, False Positives, True Negatives, False Negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "bd892db60c7761bace1ca06b9361a045",
     "grade": false,
     "grade_id": "cell-aab0f82d6a21bea5",
     "locked": false,
     "schema_version": 1,
     "solution": true
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def contigency_matrix(true_y, predicted_y):\n",
    "    # YOUR CODE HERE, Create TP, FP, TN, FN\n",
    "    tp=fp=tn=fn=0\n",
    "    for true, pred in zip(true_y, predicted_y):\n",
    "        if pred == True:\n",
    "            if pred == true:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fp += 1\n",
    "        else:\n",
    "            if pred == true:\n",
    "                tn += 1\n",
    "            else:\n",
    "                fn += 1      \n",
    "    matrix = np.array(([tp, fp], [tn, fn]))\n",
    "    # Make sure your output fits the following format:\n",
    "    # matrix = np.array(([TP, FP], [TN, FN]))\n",
    "    return matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b2f901219adca9d441082d3fcf702e6b",
     "grade": false,
     "grade_id": "cell-b3c52de1970e361e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "* Implement a function that computes accuracy (without using any built-in accuracy function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "f6705877fef7cd74d89832d32a8536a0",
     "grade": false,
     "grade_id": "cell-2e0cc734628dd4c2",
     "locked": false,
     "schema_version": 1,
     "solution": true
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def accuracy(true_y, predicted_y):\n",
    "    matrix = contigency_matrix(true_y, predicted_y)\n",
    "    tp = matrix[0][0]\n",
    "    fp = matrix[0][1]\n",
    "    tn = matrix[1][0]\n",
    "    fn = matrix[1][1]\n",
    "    if tp+fp+fn+tn == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        accuracy = (tp+tn)/(tp+fp+fn+tn)\n",
    "        return accuracy\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "4b3798ba720235065011afa2736346a7",
     "grade": false,
     "grade_id": "cell-d045e95a552112ac",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "* Implement a function that computes precision  (without using any built-in precision function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "40f4c7470d6073739cff4934761469c8",
     "grade": false,
     "grade_id": "cell-a403be8ac0ee7af0",
     "locked": false,
     "schema_version": 1,
     "solution": true
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def precision(true_y, predicted_y):\n",
    "    matrix = contigency_matrix(true_y, predicted_y)\n",
    "    tp = matrix[0][0]\n",
    "    fp = matrix[0][1]\n",
    "    tn = matrix[1][0]\n",
    "    fn = matrix[1][1]\n",
    "    if tp+fp == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        precision = tp/(tp+fp)\n",
    "        return precision\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "6dbe4298af11247615077250b24d3f59",
     "grade": false,
     "grade_id": "cell-4822b32d0cedb0e8",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "* Implement a function that computes recall (without using any built-in recall function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "cf41435f0b5003e31ae61340d3188bde",
     "grade": false,
     "grade_id": "cell-075963e37ff41c66",
     "locked": false,
     "schema_version": 1,
     "solution": true
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def recall(true_y, predicted_y):\n",
    "    matrix = contigency_matrix(true_y, predicted_y)\n",
    "    tp = matrix[0][0]\n",
    "    fp = matrix[0][1]\n",
    "    tn = matrix[1][0]\n",
    "    fn = matrix[1][1]\n",
    "    if tp+fn == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        recall = tp/(tp+fn)\n",
    "        return recall\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "c0fa7903c66407fa515b5c4c5e12b266",
     "grade": false,
     "grade_id": "cell-f0c5ff30db6fc1b0",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "* Implement a function that computes f1 (without using any built-in f1 function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "0b8095ecb0d05692b4a57cc17eff026d",
     "grade": false,
     "grade_id": "cell-bcc41b9d876ee5d4",
     "locked": false,
     "schema_version": 1,
     "solution": true
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def f1(true_y, predicted_y):\n",
    "    precision_v = precision(true_y, predicted_y)\n",
    "    recall_v = recall(true_y, predicted_y)\n",
    "    if precision_v+recall_v == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        f1 = 2*((precision_v*recall_v)/(precision_v+recall_v))\n",
    "        return f1\n",
    "                \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e0d9785e1e45e2179c318aec035059ad",
     "grade": false,
     "grade_id": "cell-c21fd73cbce64a50",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Part 3: Algorithms\n",
    "Compare the performance of Logistic Regression, Neural Networks, and SVMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "af0943a520201c48990805ce5cb2cb7c",
     "grade": false,
     "grade_id": "cell-7ea6f44ccf633c76",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "##### Logistic Regression (Lecture 3) (2pts)\n",
    "\n",
    "+ Train and test a logistic regression model\n",
    "    + Construct a table with each row being a different value of the regularization parameter and each column the aforementioned measures\n",
    "    + Explain your findings and select the optimal model\n",
    "    + Report the performance of the optimal model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "bb4b24cb9b3769517aafa02dce6321d4",
     "grade": true,
     "grade_id": "cell-eea85664ef370cd5",
     "locked": false,
     "points": 1.5,
     "schema_version": 1,
     "solution": true
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy_test</th>\n",
       "      <th>accuracy_train</th>\n",
       "      <th>f1_test</th>\n",
       "      <th>f1_train</th>\n",
       "      <th>precision_test</th>\n",
       "      <th>precision_train</th>\n",
       "      <th>recall_test</th>\n",
       "      <th>recall_train</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C-value</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0001</th>\n",
       "      <td>0.883295</td>\n",
       "      <td>0.901781</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.030151</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.016304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.0010</th>\n",
       "      <td>0.892449</td>\n",
       "      <td>0.906361</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.0100</th>\n",
       "      <td>0.892449</td>\n",
       "      <td>0.906361</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.1000</th>\n",
       "      <td>0.892449</td>\n",
       "      <td>0.906361</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005405</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0000</th>\n",
       "      <td>0.892449</td>\n",
       "      <td>0.906870</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.016129</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10.0000</th>\n",
       "      <td>0.892449</td>\n",
       "      <td>0.906870</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.021390</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.010870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100.0000</th>\n",
       "      <td>0.892449</td>\n",
       "      <td>0.906870</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.021390</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.010870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000.0000</th>\n",
       "      <td>0.892449</td>\n",
       "      <td>0.907125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.026667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.013587</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           accuracy_test  accuracy_train  f1_test  f1_train  precision_test  \\\n",
       "C-value                                                                       \n",
       "0.0001          0.883295        0.901781      0.0  0.030151             0.0   \n",
       "0.0010          0.892449        0.906361      0.0  0.000000             0.0   \n",
       "0.0100          0.892449        0.906361      0.0  0.000000             0.0   \n",
       "0.1000          0.892449        0.906361      0.0  0.005405             0.0   \n",
       "1.0000          0.892449        0.906870      0.0  0.016129             0.0   \n",
       "10.0000         0.892449        0.906870      0.0  0.021390             0.0   \n",
       "100.0000        0.892449        0.906870      0.0  0.021390             0.0   \n",
       "1000.0000       0.892449        0.907125      0.0  0.026667             0.0   \n",
       "\n",
       "           precision_train  recall_test  recall_train  \n",
       "C-value                                                \n",
       "0.0001            0.200000          0.0      0.016304  \n",
       "0.0010            0.000000          0.0      0.000000  \n",
       "0.0100            0.000000          0.0      0.000000  \n",
       "0.1000            0.500000          0.0      0.002717  \n",
       "1.0000            0.750000          0.0      0.008152  \n",
       "10.0000           0.666667          0.0      0.010870  \n",
       "100.0000          0.666667          0.0      0.010870  \n",
       "1000.0000         0.714286          0.0      0.013587  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "y = np.array(y)\n",
    "y = y.reshape(-1,1)\n",
    "X_train,X_test,y_train,y_test = train_test_split(X, y, test_size=0.1)\n",
    "imp_median_X = SimpleImputer(missing_values=np.nan, strategy='median').fit(X_train)\n",
    "X_train = imp_median_X.transform(X_train)\n",
    "X_test = imp_median_X.transform(X_test)\n",
    "\n",
    "imp_median_y = SimpleImputer(missing_values=np.nan, strategy='median').fit(y_train)\n",
    "y_train = imp_median_y.transform(y_train)\n",
    "y_test = imp_median_y.transform(y_test)\n",
    "\n",
    "# fit scaler and scale features\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train) \n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    \n",
    "def compute_scores(X_train,X_test,y_train,y_test, C):\n",
    "    # fit logistic regression model\n",
    "    logreg = LogisticRegression(C=C, solver='liblinear').fit(X_train,y_train.ravel())\n",
    "    # predict y for train set\n",
    "    pred_train = logreg.predict(X_train).tolist()\n",
    "    # predict y for test set\n",
    "    pred_test = logreg.predict(X_test).tolist()\n",
    "            \n",
    "    # calculate evaluation measures\n",
    "    evaluation_measures = dict()\n",
    "    evaluation_measures['accuracy_train'] = accuracy(y_train, pred_train)\n",
    "    evaluation_measures['accuracy_test'] = accuracy(y_test, pred_test)\n",
    "    \n",
    "    evaluation_measures['precision_train'] = precision(y_train, pred_train)\n",
    "    evaluation_measures['precision_test'] = precision(y_test, pred_test)\n",
    "    \n",
    "    evaluation_measures['recall_train'] = recall(y_train, pred_train)\n",
    "    evaluation_measures['recall_test'] = recall(y_test, pred_test)\n",
    "    \n",
    "    evaluation_measures['f1_train'] = f1(y_train, pred_train)\n",
    "    evaluation_measures['f1_test'] = f1(y_test, pred_test)\n",
    "    \n",
    "    return evaluation_measures\n",
    "    \n",
    "C = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "measures = pd.DataFrame()\n",
    "for c in C:\n",
    "    em = compute_scores(X_train_scaled,X_test_scaled,y_train,y_test, c)\n",
    "    em = pd.Series(em)\n",
    "    measures = measures.append(em, ignore_index=True)\n",
    "measures.index = C\n",
    "measures.index = measures.index.rename('C-value')\n",
    "display(measures)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "0d74b83486a9bcdcbdf539376bd56e52",
     "grade": false,
     "grade_id": "cell-f57d340c8ac3f3b6",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "* Explain what you observe regarding the positive class; i.e. the performance of the algorithm in predicting defaults. Explain why is this happening."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "106a108640959c5f8b9523216dfc1ca2",
     "grade": true,
     "grade_id": "cell-75527f6a11293f5c",
     "locked": false,
     "points": 0.5,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "<span style=\"color:blue\">**As can be seen in the results above. With the given set there are very little predicted ‘trues’.\n",
    "Since, the dataset has a lot of skewed data, meaning that the amount of positives is relatively low compared to the amount of negatives. Therefore, it makes sense that the the algorithm will return mostly false values, because every given datapoint within the dataset is most (around 90%) likely to be negative. Which results in a very low F1 score, even with an extremely high C-value, even though the accuracy is very high, since around 90% of the data is predicted correctly, the performance of the model is very poor, because it is not able to correctly predict positive values.\n",
    "**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "847a1e00ad9d4316b9512efe1a66df26",
     "grade": false,
     "grade_id": "cell-f1d84ada7bb6859e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "There are a number of ways to fix the problem you have observed above. Here we will consider two of them: downsampling and upsampling. In an ideal situation you will like your dataset to be balanced, i.e. to have the same number of instances for the positive and the negative class.\n",
    "\n",
    "**Downsampling**: Let's assume that the positive class has *n1* instances, while the negative class *n2* instances, where *n2* is much bigger than *n1*. One solution is to create a new training set for which from the *n2* instances of the negative class you sample *n1* of them only to include in your training set; hence now you have *n1* + *n1* training instances.\n",
    "\n",
    "**Upsampling**: Let's assume that the positive class has *n1* instances, while the negative class *n2* instances, where *n2* is much bigger than *n1*. Another solution is to create a new training set for which you create  *n2* instances of the positive class. To do so you sample *n2* instances from the *n1* instance, with replacement. With replacement means that you allow the same instance to be sampled multiple times; hence now you have *n2* + *n2* training instances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "#### Downsampling (OPTIONAL – If you wish to skip downsampling continue to Neural Networks further below)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "ea6ec4f362d483b9439040f12d09b46a",
     "grade": false,
     "grade_id": "cell-59aa6ae849b90374",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "* Implement a function for downsampling (**optional**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "005c2fee650b5518f0d377e26d46615d",
     "grade": true,
     "grade_id": "cell-f111bc027ec54669",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def downsample(y_train):\n",
    "    # y_train is the 1d matrix of the labels in your training data, e.g.\n",
    "    #       0     1     2     3     4   5     6     7     8   ... \n",
    "    # y = [True False False False True True False False False ... False]\n",
    "    #\n",
    "    # the function returns the position of the training data to be considered for the final training set.\n",
    "    # e.g. if you decide from the True instances to select 0, 4 and 5, while from the False instances 1, 3, and 8\n",
    "    # the outcome of the function will be [0, 1, 3, 4, 5, 8] (= sampled_indexes)\n",
    "    trues = 0\n",
    "    false_indexes = []\n",
    "    true_indexes = []\n",
    "    for index, value in enumerate(y_train):\n",
    "        if value == True:\n",
    "            trues += 1\n",
    "            true_indexes.append(index)\n",
    "        else:\n",
    "            false_indexes.append(index)\n",
    "    sampled_indexes = random.sample(false_indexes, trues) + true_indexes\n",
    "\n",
    "    return sampled_indexes\n",
    "    \n",
    "def new_training_set(X_train, y_train, sampled_indexes):\n",
    "    X_train_new = []\n",
    "    y_train_new = []\n",
    "    for index in sampled_indexes:\n",
    "        X_train_new.append(X_train[index])\n",
    "        y_train_new.append(y_train[index])\n",
    "    return [np.array(X_train_new), np.array(y_train_new)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "7acfd4fa50b0ce7567f3622b8df52dc9",
     "grade": false,
     "grade_id": "cell-0489481a9fc804d9",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "* Test the performance of logistic regression using the new training set, and report your conclusions (**optional**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "8ef852228cb6d7212a28012d7e335ad1",
     "grade": true,
     "grade_id": "cell-cd7143b6e088d522",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy_test</th>\n",
       "      <th>accuracy_train</th>\n",
       "      <th>f1_test</th>\n",
       "      <th>f1_train</th>\n",
       "      <th>precision_test</th>\n",
       "      <th>precision_train</th>\n",
       "      <th>recall_test</th>\n",
       "      <th>recall_train</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C-value</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0001</th>\n",
       "      <td>0.100686</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.182952</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.100686</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.0010</th>\n",
       "      <td>0.226545</td>\n",
       "      <td>0.540431</td>\n",
       "      <td>0.206573</td>\n",
       "      <td>0.670531</td>\n",
       "      <td>0.115183</td>\n",
       "      <td>0.522590</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.935310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.0100</th>\n",
       "      <td>0.517162</td>\n",
       "      <td>0.583558</td>\n",
       "      <td>0.203774</td>\n",
       "      <td>0.608365</td>\n",
       "      <td>0.122172</td>\n",
       "      <td>0.574163</td>\n",
       "      <td>0.613636</td>\n",
       "      <td>0.646900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.1000</th>\n",
       "      <td>0.569794</td>\n",
       "      <td>0.613208</td>\n",
       "      <td>0.210084</td>\n",
       "      <td>0.620872</td>\n",
       "      <td>0.128866</td>\n",
       "      <td>0.608808</td>\n",
       "      <td>0.568182</td>\n",
       "      <td>0.633423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0000</th>\n",
       "      <td>0.572082</td>\n",
       "      <td>0.644205</td>\n",
       "      <td>0.204255</td>\n",
       "      <td>0.645161</td>\n",
       "      <td>0.125654</td>\n",
       "      <td>0.643432</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.646900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10.0000</th>\n",
       "      <td>0.569794</td>\n",
       "      <td>0.665768</td>\n",
       "      <td>0.210084</td>\n",
       "      <td>0.664865</td>\n",
       "      <td>0.128866</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.568182</td>\n",
       "      <td>0.663073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100.0000</th>\n",
       "      <td>0.528604</td>\n",
       "      <td>0.690027</td>\n",
       "      <td>0.176000</td>\n",
       "      <td>0.682320</td>\n",
       "      <td>0.106796</td>\n",
       "      <td>0.699717</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.665768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000.0000</th>\n",
       "      <td>0.537757</td>\n",
       "      <td>0.696765</td>\n",
       "      <td>0.178862</td>\n",
       "      <td>0.690509</td>\n",
       "      <td>0.108911</td>\n",
       "      <td>0.705056</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.676550</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           accuracy_test  accuracy_train   f1_test  f1_train  precision_test  \\\n",
       "C-value                                                                        \n",
       "0.0001          0.100686        0.500000  0.182952  0.666667        0.100686   \n",
       "0.0010          0.226545        0.540431  0.206573  0.670531        0.115183   \n",
       "0.0100          0.517162        0.583558  0.203774  0.608365        0.122172   \n",
       "0.1000          0.569794        0.613208  0.210084  0.620872        0.128866   \n",
       "1.0000          0.572082        0.644205  0.204255  0.645161        0.125654   \n",
       "10.0000         0.569794        0.665768  0.210084  0.664865        0.128866   \n",
       "100.0000        0.528604        0.690027  0.176000  0.682320        0.106796   \n",
       "1000.0000       0.537757        0.696765  0.178862  0.690509        0.108911   \n",
       "\n",
       "           precision_train  recall_test  recall_train  \n",
       "C-value                                                \n",
       "0.0001            0.500000     1.000000      1.000000  \n",
       "0.0010            0.522590     1.000000      0.935310  \n",
       "0.0100            0.574163     0.613636      0.646900  \n",
       "0.1000            0.608808     0.568182      0.633423  \n",
       "1.0000            0.643432     0.545455      0.646900  \n",
       "10.0000           0.666667     0.568182      0.663073  \n",
       "100.0000          0.699717     0.500000      0.665768  \n",
       "1000.0000         0.705056     0.500000      0.676550  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sampled_indexes = downsample(y_train)\n",
    "v = new_training_set(X_train_scaled, y_train, sampled_indexes)\n",
    "X_train_down = v[0]\n",
    "y_train_down = v[1]\n",
    "\n",
    "C = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "measures = pd.DataFrame()\n",
    "for c in C:\n",
    "    em = compute_scores(X_train_down,X_test_scaled,y_train_down,y_test, c)\n",
    "    em = pd.Series(em)\n",
    "    measures = measures.append(em, ignore_index=True)\n",
    "measures.index = C\n",
    "measures.index = measures.index.rename('C-value')\n",
    "display(measures)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "checksum": "106a108640959c5f8b9523216dfc1ca2",
     "grade": true,
     "grade_id": "cell-75527f6a11293f5c",
     "locked": false,
     "points": 0.5,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "<span style=\"color:blue\">**With the downsampled training data we found much better results for predicting the test data. Overall, the logistic regression model performed the best with a C-value of 0.1. The increase of F1 scores compared to not down sampled data is significant.\n",
    "**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The last few questions below are not optional!\n",
    "If you did not finish the optional downsampling, just go through with the data created before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "fdac93da6191896d74c7346d2e875a84",
     "grade": false,
     "grade_id": "cell-4dc578274728380b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "##### Neural Network (Lecture 4) (2pts)\n",
    "\n",
    "+ Train and test a Neural Network model\n",
    "    + Construct a table with each row being a different configuration of the network (play with the number of hidden layers, the number of neurons in each layer, and the activation function) and each column the evaluation measures\n",
    "    + Explain your findings and select the optimal model\n",
    "    + Report the performance of the optimal model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "167eed2eaafde3d4951aef9059e06a58",
     "grade": true,
     "grade_id": "cell-c6f93c6c6f633c47",
     "locked": false,
     "points": 1.5,
     "schema_version": 1,
     "solution": true
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>accuracy_train</th>\n",
       "      <th>accuracy_test</th>\n",
       "      <th>precision_train</th>\n",
       "      <th>precision_test</th>\n",
       "      <th>recall_train</th>\n",
       "      <th>recall_test</th>\n",
       "      <th>f1_train</th>\n",
       "      <th>f1_test</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>activation function</th>\n",
       "      <th>nodes per layer</th>\n",
       "      <th>layers</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"9\" valign=\"top\">logistic</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">30</th>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.588101</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.144385</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.574468</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.230769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.505721</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.110599</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.510638</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.181818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.574371</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.136126</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.553191</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.218487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">100</th>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.553776</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.144231</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.638298</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.235294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.546911</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.131707</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.574468</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.214286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.549199</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.113402</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.468085</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.182573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">200</th>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.574371</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.139896</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.574468</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.225000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.542334</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.574468</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.212598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.546911</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.120603</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.510638</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.195122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"9\" valign=\"top\">tanh</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">30</th>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.533181</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.117073</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.510638</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.190476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.572082</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.131579</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.531915</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.210970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.556064</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.130653</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.553191</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.211382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">100</th>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.588101</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.158974</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.659574</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.256198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.583524</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.139037</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.553191</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.222222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.574371</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.132275</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.531915</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.211864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">200</th>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.558352</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.135000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.574468</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.218623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.576659</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.132979</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.531915</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.212766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.558352</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.127551</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.531915</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.205761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"9\" valign=\"top\">relu</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">30</th>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.533181</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.134884</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.617021</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.221374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.498856</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.112613</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.531915</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.185874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.549199</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.132353</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.574468</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.215139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">100</th>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.588101</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.144385</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.574468</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.230769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.576659</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.129032</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.510638</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.206009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.576659</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.140625</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.574468</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.225941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">200</th>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.567506</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.134021</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.553191</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.215768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.606407</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.150838</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.574468</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.238938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.583524</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.139037</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.553191</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.222222</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            accuracy_train  accuracy_test  \\\n",
       "activation function nodes per layer layers                                  \n",
       "logistic            30              1                  1.0       0.588101   \n",
       "                                    2                  1.0       0.505721   \n",
       "                                    3                  1.0       0.574371   \n",
       "                    100             1                  1.0       0.553776   \n",
       "                                    2                  1.0       0.546911   \n",
       "                                    3                  1.0       0.549199   \n",
       "                    200             1                  1.0       0.574371   \n",
       "                                    2                  1.0       0.542334   \n",
       "                                    3                  1.0       0.546911   \n",
       "tanh                30              1                  1.0       0.533181   \n",
       "                                    2                  1.0       0.572082   \n",
       "                                    3                  1.0       0.556064   \n",
       "                    100             1                  1.0       0.588101   \n",
       "                                    2                  1.0       0.583524   \n",
       "                                    3                  1.0       0.574371   \n",
       "                    200             1                  1.0       0.558352   \n",
       "                                    2                  1.0       0.576659   \n",
       "                                    3                  1.0       0.558352   \n",
       "relu                30              1                  1.0       0.533181   \n",
       "                                    2                  1.0       0.498856   \n",
       "                                    3                  1.0       0.549199   \n",
       "                    100             1                  1.0       0.588101   \n",
       "                                    2                  1.0       0.576659   \n",
       "                                    3                  1.0       0.576659   \n",
       "                    200             1                  1.0       0.567506   \n",
       "                                    2                  1.0       0.606407   \n",
       "                                    3                  1.0       0.583524   \n",
       "\n",
       "                                            precision_train  precision_test  \\\n",
       "activation function nodes per layer layers                                    \n",
       "logistic            30              1                   1.0        0.144385   \n",
       "                                    2                   1.0        0.110599   \n",
       "                                    3                   1.0        0.136126   \n",
       "                    100             1                   1.0        0.144231   \n",
       "                                    2                   1.0        0.131707   \n",
       "                                    3                   1.0        0.113402   \n",
       "                    200             1                   1.0        0.139896   \n",
       "                                    2                   1.0        0.130435   \n",
       "                                    3                   1.0        0.120603   \n",
       "tanh                30              1                   1.0        0.117073   \n",
       "                                    2                   1.0        0.131579   \n",
       "                                    3                   1.0        0.130653   \n",
       "                    100             1                   1.0        0.158974   \n",
       "                                    2                   1.0        0.139037   \n",
       "                                    3                   1.0        0.132275   \n",
       "                    200             1                   1.0        0.135000   \n",
       "                                    2                   1.0        0.132979   \n",
       "                                    3                   1.0        0.127551   \n",
       "relu                30              1                   1.0        0.134884   \n",
       "                                    2                   1.0        0.112613   \n",
       "                                    3                   1.0        0.132353   \n",
       "                    100             1                   1.0        0.144385   \n",
       "                                    2                   1.0        0.129032   \n",
       "                                    3                   1.0        0.140625   \n",
       "                    200             1                   1.0        0.134021   \n",
       "                                    2                   1.0        0.150838   \n",
       "                                    3                   1.0        0.139037   \n",
       "\n",
       "                                            recall_train  recall_test  \\\n",
       "activation function nodes per layer layers                              \n",
       "logistic            30              1                1.0     0.574468   \n",
       "                                    2                1.0     0.510638   \n",
       "                                    3                1.0     0.553191   \n",
       "                    100             1                1.0     0.638298   \n",
       "                                    2                1.0     0.574468   \n",
       "                                    3                1.0     0.468085   \n",
       "                    200             1                1.0     0.574468   \n",
       "                                    2                1.0     0.574468   \n",
       "                                    3                1.0     0.510638   \n",
       "tanh                30              1                1.0     0.510638   \n",
       "                                    2                1.0     0.531915   \n",
       "                                    3                1.0     0.553191   \n",
       "                    100             1                1.0     0.659574   \n",
       "                                    2                1.0     0.553191   \n",
       "                                    3                1.0     0.531915   \n",
       "                    200             1                1.0     0.574468   \n",
       "                                    2                1.0     0.531915   \n",
       "                                    3                1.0     0.531915   \n",
       "relu                30              1                1.0     0.617021   \n",
       "                                    2                1.0     0.531915   \n",
       "                                    3                1.0     0.574468   \n",
       "                    100             1                1.0     0.574468   \n",
       "                                    2                1.0     0.510638   \n",
       "                                    3                1.0     0.574468   \n",
       "                    200             1                1.0     0.553191   \n",
       "                                    2                1.0     0.574468   \n",
       "                                    3                1.0     0.553191   \n",
       "\n",
       "                                            f1_train   f1_test  \n",
       "activation function nodes per layer layers                      \n",
       "logistic            30              1            1.0  0.230769  \n",
       "                                    2            1.0  0.181818  \n",
       "                                    3            1.0  0.218487  \n",
       "                    100             1            1.0  0.235294  \n",
       "                                    2            1.0  0.214286  \n",
       "                                    3            1.0  0.182573  \n",
       "                    200             1            1.0  0.225000  \n",
       "                                    2            1.0  0.212598  \n",
       "                                    3            1.0  0.195122  \n",
       "tanh                30              1            1.0  0.190476  \n",
       "                                    2            1.0  0.210970  \n",
       "                                    3            1.0  0.211382  \n",
       "                    100             1            1.0  0.256198  \n",
       "                                    2            1.0  0.222222  \n",
       "                                    3            1.0  0.211864  \n",
       "                    200             1            1.0  0.218623  \n",
       "                                    2            1.0  0.212766  \n",
       "                                    3            1.0  0.205761  \n",
       "relu                30              1            1.0  0.221374  \n",
       "                                    2            1.0  0.185874  \n",
       "                                    3            1.0  0.215139  \n",
       "                    100             1            1.0  0.230769  \n",
       "                                    2            1.0  0.206009  \n",
       "                                    3            1.0  0.225941  \n",
       "                    200             1            1.0  0.215768  \n",
       "                                    2            1.0  0.238938  \n",
       "                                    3            1.0  0.222222  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.neural_network import MLPClassifier  \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X, y, test_size=0.1)\n",
    "imp_median_X = SimpleImputer(missing_values=np.nan, strategy='median').fit(X_train)\n",
    "X_train = imp_median_X.transform(X_train)\n",
    "X_test = imp_median_X.transform(X_test)\n",
    "\n",
    "imp_median_y = SimpleImputer(missing_values=np.nan, strategy='median').fit(y_train)\n",
    "y_train = imp_median_y.transform(y_train)\n",
    "y_test = imp_median_y.transform(y_test)\n",
    "\n",
    "# fit scaler and scale features\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train) \n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# downscale training data\n",
    "sampled_indexes = downsample(y_train)\n",
    "v = new_training_set(X_train_scaled, y_train, sampled_indexes)\n",
    "X_train_down = v[0]\n",
    "y_train_down = v[1]\n",
    "\n",
    "def compute_scores_neural(X_train,X_test,y_train,y_test, layers, activation_func):\n",
    "    # fit neural network model\n",
    "    mlp = MLPClassifier(max_iter=10000, hidden_layer_sizes=layers, activation=activation_func).fit(X_train, y_train.ravel())\n",
    "    # predict y for train set\n",
    "    pred_train = mlp.predict(X_train).tolist()\n",
    "    # predict y for test set\n",
    "    pred_test = mlp.predict(X_test).tolist()\n",
    "            \n",
    "    # calculate evaluation measures\n",
    "    evaluation_measures = dict()\n",
    "    evaluation_measures['accuracy_train'] = accuracy(y_train, pred_train)\n",
    "    evaluation_measures['accuracy_test'] = accuracy(y_test, pred_test)\n",
    "    \n",
    "    evaluation_measures['precision_train'] = precision(y_train, pred_train)\n",
    "    evaluation_measures['precision_test'] = precision(y_test, pred_test)\n",
    "    \n",
    "    evaluation_measures['recall_train'] = recall(y_train, pred_train)\n",
    "    evaluation_measures['recall_test'] = recall(y_test, pred_test)\n",
    "    \n",
    "    evaluation_measures['f1_train'] = f1(y_train, pred_train)\n",
    "    evaluation_measures['f1_test'] = f1(y_test, pred_test)\n",
    "    \n",
    "    return evaluation_measures\n",
    "\n",
    "# create df\n",
    "layers = [[30],[30,30],[30,30,30],[100], [100,100],[100,100,100],[200],[200,200],[200,200,200]]\n",
    "activation_functions = ['logistic', 'tanh', 'relu']\n",
    "measuresDict = dict()\n",
    "for layer in layers:\n",
    "    for activation_func in activation_functions:\n",
    "        em = compute_scores_neural(X_train_down,X_test_scaled,y_train_down,y_test, layer, activation_func)\n",
    "        if activation_func not in measuresDict.keys():\n",
    "            measuresDict[activation_func] = {layer[0]:{len(layer): em.values()}}\n",
    "        else:\n",
    "            if layer[0] not in measuresDict[activation_func].keys():\n",
    "                measuresDict[activation_func][layer[0]] = {len(layer): em.values()}\n",
    "            else:\n",
    "                measuresDict[activation_func][layer[0]][len(layer)]= em.values()\n",
    "\n",
    "measures_ordered = {(activ_func, nodes, layers): list(values)\n",
    "    for activ_func, nodes in measuresDict.items()\n",
    "    for nodes, layers in nodes.items()\n",
    "    for layers, values in layers.items()}\n",
    "measures = pd.DataFrame(measures_ordered)\n",
    "measures = measures.T\n",
    "measures.columns = em.keys()\n",
    "measures.index.set_names(['activation function', 'nodes per layer', 'layers'], inplace=True)\n",
    "display(measures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "2412fc015a280623635f3bf03e3fde9e",
     "grade": true,
     "grade_id": "cell-b4ae750d1154f837",
     "locked": false,
     "points": 0.5,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "<span style=\"color:blue\">**After running our neural network several times with multiple different nodes/layer combinations. We have come up with several findings:\n",
    "First of all, the neural network scores relatively low on precision, and therefore also has low scores on F1. It does however score high on recall.\n",
    "Secondly, ReLu activation function seems to generate the highest scores. Therefore, we have selected this activation functions for our optimal model.\n",
    "On average, we generated the most stable results with 3 hidden layers of each 100 nodes. This combination nearly always resulted in a top 5 F1 score and often had the highest F1 score. Therefore, we have chosen to go for this combination.\n",
    "So our optimal model would be ReLu, with 3 hidden layers of each 100 nodes.\n",
    "\n",
    "**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "88788580e9ea0cc74560a55c6231466b",
     "grade": false,
     "grade_id": "cell-d7e21719abffa28b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "##### SVMs (Lecture 5) (2pts)\n",
    "\n",
    "+ Train and test a Support Vector Machine model\n",
    "    + Construct a table with each row being a different configuration of the SVM algorithm (play with the regularization parameter, and the kernel function – use linear, poly, rbf, and sigmoid) and each column the evaluation measures\n",
    "    + Explain your findings and select the optimal model\n",
    "    + Report the performance of the optimal model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "e4d7a215763017f7111a6162d214ed5e",
     "grade": true,
     "grade_id": "cell-7009d775455986ad",
     "locked": false,
     "points": 1.5,
     "schema_version": 1,
     "solution": true
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>accuracy_train</th>\n",
       "      <th>accuracy_test</th>\n",
       "      <th>precision_train</th>\n",
       "      <th>precision_test</th>\n",
       "      <th>recall_train</th>\n",
       "      <th>recall_test</th>\n",
       "      <th>f1_train</th>\n",
       "      <th>f1_test</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kernel</th>\n",
       "      <th>C</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">linear</th>\n",
       "      <th>0.001</th>\n",
       "      <td>0.581551</td>\n",
       "      <td>0.414188</td>\n",
       "      <td>0.559454</td>\n",
       "      <td>0.114695</td>\n",
       "      <td>0.767380</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.647125</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.010</th>\n",
       "      <td>0.617647</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>0.607317</td>\n",
       "      <td>0.119266</td>\n",
       "      <td>0.665775</td>\n",
       "      <td>0.634146</td>\n",
       "      <td>0.635204</td>\n",
       "      <td>0.200772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.100</th>\n",
       "      <td>0.653743</td>\n",
       "      <td>0.572082</td>\n",
       "      <td>0.648579</td>\n",
       "      <td>0.119792</td>\n",
       "      <td>0.671123</td>\n",
       "      <td>0.560976</td>\n",
       "      <td>0.659658</td>\n",
       "      <td>0.197425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.000</th>\n",
       "      <td>0.663102</td>\n",
       "      <td>0.528604</td>\n",
       "      <td>0.655612</td>\n",
       "      <td>0.112676</td>\n",
       "      <td>0.687166</td>\n",
       "      <td>0.585366</td>\n",
       "      <td>0.671018</td>\n",
       "      <td>0.188976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10.000</th>\n",
       "      <td>0.668449</td>\n",
       "      <td>0.510297</td>\n",
       "      <td>0.659898</td>\n",
       "      <td>0.112108</td>\n",
       "      <td>0.695187</td>\n",
       "      <td>0.609756</td>\n",
       "      <td>0.677083</td>\n",
       "      <td>0.189394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">rbf</th>\n",
       "      <th>0.001</th>\n",
       "      <td>0.621658</td>\n",
       "      <td>0.533181</td>\n",
       "      <td>0.610169</td>\n",
       "      <td>0.131222</td>\n",
       "      <td>0.673797</td>\n",
       "      <td>0.707317</td>\n",
       "      <td>0.640407</td>\n",
       "      <td>0.221374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.010</th>\n",
       "      <td>0.621658</td>\n",
       "      <td>0.533181</td>\n",
       "      <td>0.610169</td>\n",
       "      <td>0.131222</td>\n",
       "      <td>0.673797</td>\n",
       "      <td>0.707317</td>\n",
       "      <td>0.640407</td>\n",
       "      <td>0.221374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.100</th>\n",
       "      <td>0.622995</td>\n",
       "      <td>0.530892</td>\n",
       "      <td>0.609524</td>\n",
       "      <td>0.130631</td>\n",
       "      <td>0.684492</td>\n",
       "      <td>0.707317</td>\n",
       "      <td>0.644836</td>\n",
       "      <td>0.220532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.000</th>\n",
       "      <td>0.748663</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.742188</td>\n",
       "      <td>0.129534</td>\n",
       "      <td>0.762032</td>\n",
       "      <td>0.609756</td>\n",
       "      <td>0.751979</td>\n",
       "      <td>0.213675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10.000</th>\n",
       "      <td>0.933155</td>\n",
       "      <td>0.546911</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.093264</td>\n",
       "      <td>0.938503</td>\n",
       "      <td>0.439024</td>\n",
       "      <td>0.933511</td>\n",
       "      <td>0.153846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">poly</th>\n",
       "      <th>0.001</th>\n",
       "      <td>0.506684</td>\n",
       "      <td>0.109840</td>\n",
       "      <td>0.503411</td>\n",
       "      <td>0.095349</td>\n",
       "      <td>0.986631</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.174098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.010</th>\n",
       "      <td>0.518717</td>\n",
       "      <td>0.112128</td>\n",
       "      <td>0.509642</td>\n",
       "      <td>0.095571</td>\n",
       "      <td>0.989305</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.672727</td>\n",
       "      <td>0.174468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.100</th>\n",
       "      <td>0.556150</td>\n",
       "      <td>0.155606</td>\n",
       "      <td>0.529915</td>\n",
       "      <td>0.096059</td>\n",
       "      <td>0.994652</td>\n",
       "      <td>0.951220</td>\n",
       "      <td>0.691450</td>\n",
       "      <td>0.174497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.000</th>\n",
       "      <td>0.774064</td>\n",
       "      <td>0.656751</td>\n",
       "      <td>0.836066</td>\n",
       "      <td>0.129252</td>\n",
       "      <td>0.681818</td>\n",
       "      <td>0.463415</td>\n",
       "      <td>0.751105</td>\n",
       "      <td>0.202128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10.000</th>\n",
       "      <td>0.922460</td>\n",
       "      <td>0.583524</td>\n",
       "      <td>0.927027</td>\n",
       "      <td>0.110497</td>\n",
       "      <td>0.917112</td>\n",
       "      <td>0.487805</td>\n",
       "      <td>0.922043</td>\n",
       "      <td>0.180180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">sigmoid</th>\n",
       "      <th>0.001</th>\n",
       "      <td>0.573529</td>\n",
       "      <td>0.372998</td>\n",
       "      <td>0.550832</td>\n",
       "      <td>0.110368</td>\n",
       "      <td>0.796791</td>\n",
       "      <td>0.804878</td>\n",
       "      <td>0.651366</td>\n",
       "      <td>0.194118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.010</th>\n",
       "      <td>0.573529</td>\n",
       "      <td>0.372998</td>\n",
       "      <td>0.550832</td>\n",
       "      <td>0.110368</td>\n",
       "      <td>0.796791</td>\n",
       "      <td>0.804878</td>\n",
       "      <td>0.651366</td>\n",
       "      <td>0.194118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.100</th>\n",
       "      <td>0.562834</td>\n",
       "      <td>0.324943</td>\n",
       "      <td>0.540587</td>\n",
       "      <td>0.103125</td>\n",
       "      <td>0.836898</td>\n",
       "      <td>0.804878</td>\n",
       "      <td>0.656873</td>\n",
       "      <td>0.182825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.000</th>\n",
       "      <td>0.556150</td>\n",
       "      <td>0.510297</td>\n",
       "      <td>0.549296</td>\n",
       "      <td>0.112108</td>\n",
       "      <td>0.625668</td>\n",
       "      <td>0.609756</td>\n",
       "      <td>0.585000</td>\n",
       "      <td>0.189394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10.000</th>\n",
       "      <td>0.517380</td>\n",
       "      <td>0.530892</td>\n",
       "      <td>0.517241</td>\n",
       "      <td>0.113208</td>\n",
       "      <td>0.521390</td>\n",
       "      <td>0.585366</td>\n",
       "      <td>0.519308</td>\n",
       "      <td>0.189723</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                accuracy_train  accuracy_test  precision_train  \\\n",
       "kernel  C                                                        \n",
       "linear  0.001         0.581551       0.414188         0.559454   \n",
       "        0.010         0.617647       0.526316         0.607317   \n",
       "        0.100         0.653743       0.572082         0.648579   \n",
       "        1.000         0.663102       0.528604         0.655612   \n",
       "        10.000        0.668449       0.510297         0.659898   \n",
       "rbf     0.001         0.621658       0.533181         0.610169   \n",
       "        0.010         0.621658       0.533181         0.610169   \n",
       "        0.100         0.622995       0.530892         0.609524   \n",
       "        1.000         0.748663       0.578947         0.742188   \n",
       "        10.000        0.933155       0.546911         0.928571   \n",
       "poly    0.001         0.506684       0.109840         0.503411   \n",
       "        0.010         0.518717       0.112128         0.509642   \n",
       "        0.100         0.556150       0.155606         0.529915   \n",
       "        1.000         0.774064       0.656751         0.836066   \n",
       "        10.000        0.922460       0.583524         0.927027   \n",
       "sigmoid 0.001         0.573529       0.372998         0.550832   \n",
       "        0.010         0.573529       0.372998         0.550832   \n",
       "        0.100         0.562834       0.324943         0.540587   \n",
       "        1.000         0.556150       0.510297         0.549296   \n",
       "        10.000        0.517380       0.530892         0.517241   \n",
       "\n",
       "                precision_test  recall_train  recall_test  f1_train   f1_test  \n",
       "kernel  C                                                                      \n",
       "linear  0.001         0.114695      0.767380     0.780488  0.647125  0.200000  \n",
       "        0.010         0.119266      0.665775     0.634146  0.635204  0.200772  \n",
       "        0.100         0.119792      0.671123     0.560976  0.659658  0.197425  \n",
       "        1.000         0.112676      0.687166     0.585366  0.671018  0.188976  \n",
       "        10.000        0.112108      0.695187     0.609756  0.677083  0.189394  \n",
       "rbf     0.001         0.131222      0.673797     0.707317  0.640407  0.221374  \n",
       "        0.010         0.131222      0.673797     0.707317  0.640407  0.221374  \n",
       "        0.100         0.130631      0.684492     0.707317  0.644836  0.220532  \n",
       "        1.000         0.129534      0.762032     0.609756  0.751979  0.213675  \n",
       "        10.000        0.093264      0.938503     0.439024  0.933511  0.153846  \n",
       "poly    0.001         0.095349      0.986631     1.000000  0.666667  0.174098  \n",
       "        0.010         0.095571      0.989305     1.000000  0.672727  0.174468  \n",
       "        0.100         0.096059      0.994652     0.951220  0.691450  0.174497  \n",
       "        1.000         0.129252      0.681818     0.463415  0.751105  0.202128  \n",
       "        10.000        0.110497      0.917112     0.487805  0.922043  0.180180  \n",
       "sigmoid 0.001         0.110368      0.796791     0.804878  0.651366  0.194118  \n",
       "        0.010         0.110368      0.796791     0.804878  0.651366  0.194118  \n",
       "        0.100         0.103125      0.836898     0.804878  0.656873  0.182825  \n",
       "        1.000         0.112108      0.625668     0.609756  0.585000  0.189394  \n",
       "        10.000        0.113208      0.521390     0.585366  0.519308  0.189723  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.svm import SVC  \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X, y, test_size=0.1)\n",
    "imp_median_X = SimpleImputer(missing_values=np.nan, strategy='median').fit(X_train)\n",
    "X_train = imp_median_X.transform(X_train)\n",
    "X_test = imp_median_X.transform(X_test)\n",
    "\n",
    "imp_median_y = SimpleImputer(missing_values=np.nan, strategy='median').fit(y_train)\n",
    "y_train = imp_median_y.transform(y_train)\n",
    "y_test = imp_median_y.transform(y_test)\n",
    "\n",
    "# fit scaler and scale features\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train) \n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# downscale training data\n",
    "sampled_indexes = downsample(y_train)\n",
    "v = new_training_set(X_train_scaled, y_train, sampled_indexes)\n",
    "X_train_down = v[0]\n",
    "y_train_down = v[1]\n",
    "\n",
    "def compute_scores_SVM(X_train,X_test,y_train,y_test, C, kernel):\n",
    "    # fit neural network model\n",
    "    svc = SVC(C=C, kernel=kernel, gamma='scale').fit(X_train, y_train.ravel())\n",
    "    # predict y for train set\n",
    "    pred_train = svc.predict(X_train).tolist()\n",
    "    # predict y for test set\n",
    "    pred_test = svc.predict(X_test).tolist()\n",
    "            \n",
    "    # calculate evaluation measures\n",
    "    evaluation_measures = dict()\n",
    "    evaluation_measures['accuracy_train'] = accuracy(y_train, pred_train)\n",
    "    evaluation_measures['accuracy_test'] = accuracy(y_test, pred_test)\n",
    "    \n",
    "    evaluation_measures['precision_train'] = precision(y_train, pred_train)\n",
    "    evaluation_measures['precision_test'] = precision(y_test, pred_test)\n",
    "    \n",
    "    evaluation_measures['recall_train'] = recall(y_train, pred_train)\n",
    "    evaluation_measures['recall_test'] = recall(y_test, pred_test)\n",
    "    \n",
    "    evaluation_measures['f1_train'] = f1(y_train, pred_train)\n",
    "    evaluation_measures['f1_test'] = f1(y_test, pred_test)\n",
    "    \n",
    "    return evaluation_measures\n",
    "\n",
    "# create df\n",
    "C = [0.001, 0.01, 0.1, 1, 10]\n",
    "kernels = ['linear', 'rbf', 'poly', 'sigmoid']\n",
    "measuresDict = dict()\n",
    "for kernel in kernels:\n",
    "    for c in C:\n",
    "        em = compute_scores_SVM(X_train_down,X_test_scaled,y_train_down,y_test, c, kernel)\n",
    "        if kernel not in measuresDict.keys():\n",
    "            measuresDict[kernel] = {c: em.values()}\n",
    "        else:\n",
    "            measuresDict[kernel][c] = em.values()\n",
    "\n",
    "measures_ordered = {(kernel, c): list(values)\n",
    "    for kernel, c in measuresDict.items()\n",
    "    for c, values in c.items()}\n",
    "measures = pd.DataFrame(measures_ordered)\n",
    "measures = measures.T\n",
    "measures.columns = em.keys()\n",
    "measures.index.set_names(['kernel', 'C'], inplace=True)\n",
    "display(measures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "36cda951bae6da0700d3f6202d0f1a89",
     "grade": true,
     "grade_id": "cell-42c8bf3a6a671fef",
     "locked": false,
     "points": 0.5,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "<span style=\"color:blue\">**Our model showed the best performance with the sigmoid kernel and a regularization value of 0.1. The results were only slightly better than the linear and the rbf model. The poly model seemed to perform the worst.  Therefore we chose the sigmoid kernel. The performance had its peak at C=0.1, at higher, and lower values the performance was a bit worse.\n",
    "**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "baabfebe46559e2d055cef7ceae38b0e",
     "grade": false,
     "grade_id": "cell-23280b034d299667",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### Compare Algorithms (2pts)\n",
    "* Plot the Precision-Recall curves for the best model for each one of the above algorithms, Logistic Regression, Neural Nets, and SVM.\n",
    "    * Use the precision_recall_curve from scikit-learn\n",
    "* Explain your findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "b17597ab95452fa3d4259cb4d2bee6c2",
     "grade": true,
     "grade_id": "cell-d6967e3b3e3dbe7a",
     "locked": false,
     "points": 1.5,
     "schema_version": 1,
     "solution": true
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1a2b8791d0>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier  \n",
    "from sklearn.svm import SVC  \n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X, y, test_size=0.1)\n",
    "imp_median_X = SimpleImputer(missing_values=np.nan, strategy='median').fit(X_train)\n",
    "X_train = imp_median_X.transform(X_train)\n",
    "X_test = imp_median_X.transform(X_test)\n",
    "\n",
    "imp_median_y = SimpleImputer(missing_values=np.nan, strategy='median').fit(y_train)\n",
    "y_train = imp_median_y.transform(y_train)\n",
    "y_test = imp_median_y.transform(y_test)\n",
    "\n",
    "# fit scaler and scale features\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train) \n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# downsample for the logistic regression model\n",
    "sampled_indexes = downsample(y_train)\n",
    "v = new_training_set(X_train_scaled, y_train, sampled_indexes)\n",
    "X_train_down = v[0]\n",
    "y_train_down = v[1]\n",
    "\n",
    "# # train all models\n",
    "# logreg = LogisticRegression(C=0.1, solver='liblinear').fit(X_train_down,y_train_down.ravel())\n",
    "# mlp = MLPClassifier(max_iter=10000, hidden_layer_sizes=[100,100,100], activation='relu').fit(X_train_scaled, y_train.ravel())\n",
    "# svc = SVC(C=10, kernel='sigmoid', gamma='scale').fit(X_train_scaled, y_train.ravel())\n",
    "\n",
    "# train all models\n",
    "logreg = LogisticRegression(C=0.1, solver='liblinear').fit(X_train_down,y_train_down.ravel())\n",
    "mlp = MLPClassifier(max_iter=10000, hidden_layer_sizes=[100,100,100], activation='relu').fit(X_train_down, y_train_down.ravel())\n",
    "svc = SVC(C=0.1, kernel='sigmoid', gamma='scale').fit(X_train_down, y_train_down.ravel())\n",
    "\n",
    "pred_logreg = logreg.predict(X_test_scaled)\n",
    "pred_mlp = mlp.predict(X_test_scaled)\n",
    "pred_svc = svc.predict(X_test_scaled)\n",
    "precision_logreg, recall_logreg, thresholds_logreg = precision_recall_curve(y_test, pred_logreg)\n",
    "precision_mlp, recall_mlp, thresholds_mlp = precision_recall_curve(y_test, pred_mlp)\n",
    "precision_svc, recall_svc, thresholds_svc = precision_recall_curve(y_test, pred_svc)\n",
    "\n",
    "plt.plot(recall_logreg, precision_logreg)\n",
    "plt.plot(recall_mlp, precision_mlp)\n",
    "plt.plot(recall_svc, precision_svc)\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.legend(['Logistic regression', 'Neural network', 'SVM', 'predicted mean'], loc = 'upper right')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "58ca34644cf139f5987905e2058ea0b3",
     "grade": true,
     "grade_id": "cell-b91319845bc74219",
     "locked": false,
     "points": 0.5,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "<span style=\"color:blue\">**We found out, after running the plot several times with different train/test sets, the SVM outperformed the linear and neural network model most of the time. Especially on the recall, SVM seems to generate the highest scores, whilst not giving in on the precision compared to the others either. Therefore, since the SVM shows the most steady results, we think it is the best model of the three in this case. \n",
    "**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
