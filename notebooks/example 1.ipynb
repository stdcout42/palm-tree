{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-575272ac8c88383d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Applied Machine Learning\n",
    "## Practical Assignment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "Names: ['Alex Hakvoort', 'Sebastiaan van Dijk'] \n",
    "Studentnumbers: ['12488674', '12400319']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-JnevPdzdZo6",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e8b3a90e3eadc324",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Important Notes:\n",
    "1. Submit through **Canvas** before 11:59pm on Wednesday, April 22, 2020.\n",
    "2. No late homework will be accepted.\n",
    "3. This is a **group-of-two** assignment; hence choose **one** partner to work with.\n",
    "4. The submitted file should be in ipynb format\n",
    "5. The assignment is worth 10 points\n",
    "6. For questions, please use the discussion part of canvas (English only!)\n",
    "\n",
    "### Software:\n",
    "We will be using Python programming language throughout this course. Further we will be using:\n",
    "+ IPython Notebooks (as an environment)\n",
    "+ Numpy\n",
    "+ Pandas\n",
    "+ Scikit-learn\n",
    "\n",
    "\n",
    "### Background:\n",
    "\n",
    "This practical assignment will be covering linear regression and evaluation. For the assignment, please download a [dataset](https://drive.google.com/open?id=1rESPdl7CUfvgkA44YQ42pcOaMOCGN8B6) containing demographic information and crime statistics (in a given year) for some cities in the US.\n",
    "\n",
    "Assume that for certain cities there is missing information about crimes, so we would like to learn how to estimate the number of non-violent crimes based on characteristics of the city (demographics, location etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f55bc95980e83e55",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from nose.tools import assert_count_equal, assert_equal\n",
    "from numpy.testing import *\n",
    "from pandas.testing import assert_frame_equal\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tYWhz0bxdZo9",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8452bfe549fa98ac",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Part 1: Loading the data into a Pandas Data Frame [0.5 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The descriptions of the columns of the dataset can be found here:\n",
    "\n",
    "**pop**: population\n",
    "\n",
    "**pctUrban**: percentage of people living in areas classified as urban\n",
    "\n",
    "**medIncome**: Median Income\n",
    "\n",
    "**pct12-29**: percentage of population that is 12-21 in age\n",
    "\n",
    "**pct65up**: percentage of population that is 65 and over in age\n",
    "\n",
    "**pctPoverty**: percentage of people under the poverty level\n",
    "\n",
    "**pctAllDivorc**: percentage of population who are divorced\n",
    "\n",
    "**pctUnemploy**: percentage of people 16 and over, in the labor force, and unemployed\n",
    "\n",
    "**perHoush**: mean persons per household\n",
    "\n",
    "**pctHousOccup**: percent of housing occupied\n",
    "\n",
    "**persHomeless**: number of homeless people\n",
    "\n",
    "**persEmergShelt**: number of people in homeless shelters\n",
    "\n",
    "**nonViolPerPop**: total number of non-violent crimes per 100K popuation\n",
    "\n",
    "**State**: the state in which this town/city is located\n",
    "\n",
    "**countyCode**: the code number of the county of the state this town/city is located\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Question 1a:\n",
    "Load the data into a Pandas DataFrame. At this point, make sure that you only load the following columns: \n",
    "\n",
    "`'pop', 'pctUrban', 'medIncome', 'pct12-29', 'pct65up', 'pctPoverty', 'pctAllDivorc', 'pctUnemploy', 'perHoush', 'pctHousOccup', 'persHomeless', 'persEmergShelt', 'nonViolPerPop'`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "csv_path = 'crime_data.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9a5813b911a397d1",
     "locked": false,
     "schema_version": 1,
     "solution": true
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(949, 13)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>perHoush</th>\n",
       "      <th>pct12-29</th>\n",
       "      <th>pct65up</th>\n",
       "      <th>pctUrban</th>\n",
       "      <th>pctPoverty</th>\n",
       "      <th>pctUnemploy</th>\n",
       "      <th>pctAllDivorc</th>\n",
       "      <th>pctHousOccup</th>\n",
       "      <th>persEmergShelt</th>\n",
       "      <th>persHomeless</th>\n",
       "      <th>nonViolPerPop</th>\n",
       "      <th>medIncome</th>\n",
       "      <th>pop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.10</td>\n",
       "      <td>21.44</td>\n",
       "      <td>11.33</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.96</td>\n",
       "      <td>2.70</td>\n",
       "      <td>4.47</td>\n",
       "      <td>98.37</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>1394.59</td>\n",
       "      <td>75122.0</td>\n",
       "      <td>11980.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.82</td>\n",
       "      <td>21.30</td>\n",
       "      <td>17.18</td>\n",
       "      <td>100.0</td>\n",
       "      <td>3.98</td>\n",
       "      <td>2.43</td>\n",
       "      <td>5.42</td>\n",
       "      <td>97.15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1955.95</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23123.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.76</td>\n",
       "      <td>40.53</td>\n",
       "      <td>12.65</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.99</td>\n",
       "      <td>9.08</td>\n",
       "      <td>9.73</td>\n",
       "      <td>92.45</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>9988.79</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.60</td>\n",
       "      <td>27.41</td>\n",
       "      <td>14.42</td>\n",
       "      <td>100.0</td>\n",
       "      <td>4.01</td>\n",
       "      <td>4.85</td>\n",
       "      <td>7.64</td>\n",
       "      <td>95.11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1890.88</td>\n",
       "      <td>42805.0</td>\n",
       "      <td>28700.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.46</td>\n",
       "      <td>35.16</td>\n",
       "      <td>8.58</td>\n",
       "      <td>100.0</td>\n",
       "      <td>13.68</td>\n",
       "      <td>4.18</td>\n",
       "      <td>8.64</td>\n",
       "      <td>95.07</td>\n",
       "      <td>125</td>\n",
       "      <td>15</td>\n",
       "      <td>4747.58</td>\n",
       "      <td>NaN</td>\n",
       "      <td>74111.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   perHoush  pct12-29  pct65up  pctUrban  pctPoverty  pctUnemploy  \\\n",
       "0      3.10     21.44    11.33     100.0        1.96         2.70   \n",
       "1      2.82     21.30    17.18     100.0        3.98         2.43   \n",
       "2      2.76     40.53    12.65       0.0       29.99         9.08   \n",
       "3      2.60     27.41    14.42     100.0        4.01         4.85   \n",
       "4      2.46     35.16     8.58     100.0       13.68         4.18   \n",
       "\n",
       "   pctAllDivorc  pctHousOccup  persEmergShelt  persHomeless  nonViolPerPop  \\\n",
       "0          4.47         98.37              11             0        1394.59   \n",
       "1          5.42         97.15               0             0        1955.95   \n",
       "2          9.73         92.45               2             0        9988.79   \n",
       "3          7.64         95.11               0             0        1890.88   \n",
       "4          8.64         95.07             125            15        4747.58   \n",
       "\n",
       "   medIncome      pop  \n",
       "0    75122.0  11980.0  \n",
       "1        NaN  23123.0  \n",
       "2        NaN      NaN  \n",
       "3    42805.0  28700.0  \n",
       "4        NaN  74111.0  "
      ]
     },
     "execution_count": 482,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "\n",
    "# Load the dataset\n",
    "crime_data = pd.read_csv('./crime_data.csv',sep=\",\", header = 0, \n",
    "                 usecols=['pop', 'pctUrban', 'medIncome', 'pct12-29', 'pct65up', 'pctPoverty', 'pctAllDivorc', 'pctUnemploy', 'perHoush', 'pctHousOccup', \n",
    "                          'persHomeless', 'persEmergShelt', 'nonViolPerPop'])\n",
    "\n",
    "### END SOLUTION\n",
    "\n",
    "print(crime_data.shape)\n",
    "crime_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-793ac60698061e82",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Question 1b:\n",
    "We want to predict the number of crimes. Identify the features X and the target variable Y, and turn the X and Y DataFrames into Numpy arrays.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "_FzIEG5DdZo-",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8eb698d0c3f532fd",
     "locked": false,
     "schema_version": 1,
     "solution": true
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "\n",
    "X = crime_data[['pop', 'pctUrban', 'medIncome', 'pct12-29', 'pct65up', 'pctPoverty', 'pctAllDivorc', 'pctUnemploy', \n",
    "        'perHoush', 'pctHousOccup', 'persHomeless', 'persEmergShelt']].values\n",
    "Y = crime_data['nonViolPerPop'].values.reshape(-1,1)\n",
    "\n",
    "### END SOLUTION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P9YvTy9kdZpG",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-afc0b6593e782894",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Part 2: Split the data into a training set and a test set [0.5 pts]\n",
    "### Question 2:\n",
    "Split the data into a training and a test set. Use a  70%-30% split.  \n",
    "Print the number of examples in the training set and the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "e_oiLoaWdZpH",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-cb09606197bd3f58",
     "locked": false,
     "schema_version": 1,
     "solution": true
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7.63710e+04 9.46900e+01 4.07040e+04 2.44800e+01 1.44200e+01 4.92000e+00\n",
      "  7.66000e+00 5.01000e+00 2.79000e+00 7.67300e+01 0.00000e+00 9.60000e+01]\n",
      " [1.82044e+05 1.00000e+02 1.97790e+04 2.96300e+01 1.20100e+01 2.65300e+01\n",
      "  1.75300e+01 1.03900e+01 2.51000e+00 9.04200e+01 5.00000e+00 4.06000e+02]\n",
      " [1.02270e+04 0.00000e+00 2.88920e+04 2.42400e+01 1.35800e+01 7.95000e+00\n",
      "  9.24000e+00 3.60000e+00 2.57000e+00 9.78600e+01 0.00000e+00 0.00000e+00]\n",
      " [3.51840e+04 1.00000e+02         nan 2.00900e+01 1.14900e+01 1.15000e+00\n",
      "  4.16000e+00 2.25000e+00 2.95000e+00 9.74300e+01 0.00000e+00 0.00000e+00]\n",
      " [1.60640e+04 1.00000e+02         nan 2.38700e+01 1.42000e+01 5.71000e+00\n",
      "  1.07300e+01 3.93000e+00 2.52000e+00 9.65300e+01 0.00000e+00 1.90000e+01]]\n",
      "[[2.6542e+04 8.6820e+01 3.2596e+04 2.4090e+01 1.5500e+01 6.7300e+00\n",
      "  8.7500e+00 5.1000e+00 2.6600e+00 9.6250e+01 0.0000e+00 0.0000e+00]\n",
      " [3.0954e+04 1.0000e+02 3.7856e+04 2.5430e+01 1.0450e+01 6.2200e+00\n",
      "  9.7500e+00 4.1700e+00 2.6900e+00 9.4850e+01 0.0000e+00 0.0000e+00]\n",
      " [1.9899e+04 1.0000e+02 3.1250e+04 2.7850e+01 1.0820e+01 9.5800e+00\n",
      "  1.3930e+01 7.4000e+00 2.7900e+00 9.7450e+01 0.0000e+00 0.0000e+00]\n",
      " [2.5067e+04 1.0000e+02 5.8995e+04 2.2820e+01 1.4880e+01 3.0400e+00\n",
      "  4.5600e+00 3.6800e+00 3.2200e+00 9.8530e+01 0.0000e+00 1.9000e+01]\n",
      " [2.1853e+04 1.0000e+02 3.6096e+04 2.7140e+01 1.0190e+01 6.5100e+00\n",
      "  9.8300e+00 4.2800e+00 2.5700e+00 9.6730e+01 0.0000e+00 0.0000e+00]]\n"
     ]
    }
   ],
   "source": [
    "X_train = 'Replace this string with the correct answer'\n",
    "X_test = 'Replace this string with the correct answer'\n",
    "Y_train = 'Replace this string with the correct answer'\n",
    "Y_test = 'Replace this string with the correct answer'\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.3)\n",
    "\n",
    "### END SOLUTION\n",
    "print(X_train[:5])\n",
    "print(X_test[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZIYFfQN0dZpM",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-473a187a1ce777d3",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Part 3: Linear Regression [2 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "prbPvQZodZpN",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5db55cc890570964",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "**Missing Data**: Often the data you are considering is incomplete. For example in some city, the number of homeless people might be unknown. In this case, if you look into the datasets you will find the value *NaN*. This is not a real value, hence Linear Regression cannot handle it.\n",
    "\n",
    "The question is how can we handle missing data. There are many ways to do so, some more sophisticated than others. Here we will use a simple approach. This simple approach fills in the missing values, i.e. replaces the *NaN* by the median of the corresponding feature. E.g. if there is a *NaN* value for the population in one city, this *NaN* value will be replaced by the median number of the population in all other cities in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "DZEv4VfCdZpP",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0777a154778f1fe3",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "# Fill in the missing data in the dataset (i.e. replace NaN values) \n",
    "imp_median_X = SimpleImputer(missing_values=np.nan, strategy='median').fit(X_train)\n",
    "X_train = imp_median_X.transform(X_train)\n",
    "X_test = imp_median_X.transform(X_test)\n",
    "\n",
    "imp_median_Y = SimpleImputer(missing_values=np.nan, strategy='median').fit(Y_train)\n",
    "Y_train = imp_median_Y.transform(Y_train)\n",
    "Y_test = imp_median_Y.transform(Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5f6424ae3678e3c9",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Question 3a:\n",
    "Train a linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "BQc0v7GCdZpV",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-40087f3cfc1918c7",
     "locked": false,
     "schema_version": 1,
     "solution": true
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.78531897e-03  9.48309549e+00 -2.93683465e-03 -3.57523539e+01\n",
      "   3.07080621e+01  1.45924641e+02  3.04383597e+02 -1.12296606e+02\n",
      "   5.21315760e+02 -5.20501987e+01 -5.49403705e+00  3.51881470e+00]]\n"
     ]
    }
   ],
   "source": [
    "### BEGIN SOLUTION\n",
    "def train_linregmodel(X, Y):\n",
    "    lr = LinearRegression().fit(X, Y)    \n",
    "    return lr\n",
    "### END SOLUTION\n",
    "    \n",
    "\n",
    "lr = train_linregmodel(X_train,Y_train)\n",
    "print(lr.coef_)\n",
    "# print(\"lr.intercept_: {}\".format(lr.intercept_))\n",
    "# print(\"R^2_train = %f\" % lr.score(X_train, Y_train))\n",
    "# print(\"R^2_test = %f\" % lr.score(X_test,Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-71f736155f84e035",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Question 3b:\n",
    "Compare the importance of features based on the parameters $\\theta$ of the model. \n",
    "\n",
    "1. Which are the top-5 most important features for the predicting the number of crimes per capita?   \n",
    "2. Out of those, which correlate positively and which negatively with the number of crimes in a region?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3b020283aa02adae",
     "locked": false,
     "schema_version": 1,
     "solution": true
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pop                -0.002785\n",
      "pctUrban            9.483095\n",
      "medIncome          -0.002937\n",
      "pct12-29          -35.752354\n",
      "pct65up            30.708062\n",
      "pctPoverty        145.924641\n",
      "pctAllDivorc      304.383597\n",
      "pctUnemploy      -112.296606\n",
      "perHoush          521.315760\n",
      "pctHousOccup      -52.050199\n",
      "persHomeless       -5.494037\n",
      "persEmergShelt      3.518815\n",
      "dtype: float64\n",
      "['perHoush', 'pctAllDivorc', 'pctPoverty', 'pctUnemploy', 'pctHousOccup']\n"
     ]
    }
   ],
   "source": [
    "coefficients = pd.Series(lr.coef_[0], index=['pop', 'pctUrban', 'medIncome', 'pct12-29', 'pct65up', \n",
    "                                                'pctPoverty', 'pctAllDivorc', 'pctUnemploy', 'perHoush',\n",
    "                                                'pctHousOccup', 'persHomeless', 'persEmergShelt'])\n",
    "coefficientsABS = coefficients.abs()\n",
    "print(coefficients)\n",
    "top5 = coefficientsABS.nlargest(n=5).index.tolist()\n",
    "print(top5)\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e16W9xIcdZpg"
   },
   "source": [
    "<span style=\"color:blue\">**1:'perHoush', 'pctAllDivorc', 'pctPoverty', 'pctUnemploy', 'pctHousOccup' are the most important features for predicting the number of crimes per capita. 2:\n",
    "Out of those, 'pctUnemploy' and 'pctHousOccup' correlate negatively with the number of crimes in a region. The rest of the features correlate positively.**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-02ef93cfecd6a92f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Question 3c:\n",
    "\n",
    "Compute the Mean Absolute Error and Root Mean Squared Error, do this without using the scikit-learn API for these values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "D1wWlXgcdZpk",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d9e0e18c6ac6a823",
     "locked": false,
     "schema_version": 1,
     "solution": true
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def rmse(y_true, y_pred):\n",
    "    ### BEGIN SOLUTION\n",
    "    MSE = 0\n",
    "    n = 0\n",
    "    for yt, yp in zip(y_true, y_pred):\n",
    "        MSE += (yp[0] - yt[0])**2\n",
    "        n += 1\n",
    "    RMSE = (MSE/n)**0.5\n",
    "    \n",
    "    ### END SOLUTION\n",
    "    return RMSE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def mae(y_true, y_pred):\n",
    "    ### BEGIN SOLUTION\n",
    "    MAE = 0\n",
    "    n = 0\n",
    "    for yt, yp in zip(y_true, y_pred):\n",
    "        MAE += abs(yp[0] - yt[0])\n",
    "        n += 1\n",
    "    \n",
    "    ### END SOLUTION\n",
    "    return (MAE/n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h-htxVRadZpp",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ff9572a7060b40ee",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Question 3d:\n",
    "MAE is an L1 error, while RMSE is an L2 error (L2 errors are based on the squared errors of each prediction in contrast to L1). Consider a scenario where having a relatively small deviation from the true value is ok, but you would really like to avoid making large errors in predictions. Which of those two metrics would you choose to evaluate your models? Explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zCr2CzPNdZpx"
   },
   "source": [
    "<span style=\"color:blue\">**When predicting the amount of infections in a virus outbreak, you want to minimize the deviation from the truth value in order to take good action. When you think that there are 100 infections and in reality there are 110, this is a small error which should not have a big impact on the actions to be taken. If the propagation rate for the virus is 3 this means that every infected person is probably going to infect 3 more people. If we say that the virus will be transmittable one week after infection. This means that when you want to make a prediction of how the disease will spread after a months time, this mistake of 10 would result in a wrong prediction of 810 people (10 x 3^4). This error will only increase the further the prediction will go after three months (let's say twelve week) it will be (10 x 3^12) = 5314410. \n",
    "So in this case, the error at the start of 10 infections is not a good situation however it’s acceptable/okay. However, it will lead to large errors in predictions that you want to avoid.\n",
    "\n",
    "In order to reduce the large errors in predictions, we want to use RMSE (L2 error) Because : ‘L2 errors are based on the squared errors of each prediction in contrast to L1’. In the case of MAE the impact of the error is linear with the size of the error, while for the RMSE, errors worsen the outcome of the RMSE exponentially, making large errors very impactful.**</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "waQHGkwzdZpy",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-880a51e909914143",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Part 4. Adding features [2 pts]\n",
    "1. Add a number of features by including polynomials and interactions of different degree\n",
    "2. Train and test the different linear regression models over the data\n",
    "3. Test whether increasing the complexity of the model overfits the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PZCylJThdZqD",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4c953b9943fce092",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Question 4a\n",
    "Implement a function that constructs additional features by considering the polynomials of the original features along with their interactions. Degree is the degree of the polynomial.  \n",
    "Consider the original dataset that was loaded into Pandas, and then turned into a Numpy array from Part 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "UPxirRI_dZqF",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0b1c89bb3ea71cd3",
     "locked": false,
     "schema_version": 1,
     "solution": true
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def polynomial(X, degree):\n",
    "    \n",
    "    ### BEGIN SOLUTION\n",
    "    poly = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "    X_poly = poly.fit_transform(X)\n",
    "    ### END SOLUTION\n",
    "    return X_poly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lSmT1E-TdZqV",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9962d8a34afca4c5",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Question 4b:\n",
    "Write your conclusions regarding the performance of the models of increasing complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "OJ8eBIbOdZqM",
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-dc744bd0d05ecf76",
     "locked": false,
     "points": 2,
     "schema_version": 1,
     "solution": true
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For degree 1, the RMSE for the training set is 1798.4014153708256 and for the test set is 1825.609107683881\n",
      "For degree 1, the R^2 score for the training set is 0.438453051958404 and for the test set is 0.3279896140843841\n",
      "For degree 2, the RMSE for the training set is 1561.0069015556755 and for the test set is 10322.795046215322\n",
      "For degree 2, the R^2 score for the training set is 0.5769200481839324 and for the test set is -20.48597208603745\n",
      "For degree 3, the RMSE for the training set is 1656.246134447975 and for the test set is 879742.9111023739\n",
      "For degree 3, the R^2 score for the training set is 0.5237197694212452 and for the test set is -156051.93807680055\n",
      "For degree 4, the RMSE for the training set is 2003.95192399816 and for the test set is 10148520.466712318\n",
      "For degree 4, the R^2 score for the training set is 0.30275180840990734 and for the test set is -20766621.17685422\n",
      "For degree 5, the RMSE for the training set is 2615.429622583414 and for the test set is 23170511.08822697\n",
      "For degree 5, the R^2 score for the training set is -0.18767845002853867 and for the test set is -108250926.11975318\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1a25ac41d0>"
      ]
     },
     "execution_count": 491,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "RMSE_scores_train = []\n",
    "RMSE_scores_test = []\n",
    "# Generate polynomial dataset (both training and test) of degrees 1, 2, 3, 4, 5\n",
    "for power in [1, 2, 3, 4, 5]:\n",
    "    X_train_poly = polynomial(X_train, power)\n",
    "    X_test_poly = polynomial(X_test, power)\n",
    "# Scale all features using the RobustScaler\n",
    "    scaler = RobustScaler().fit(X_train_poly)\n",
    "    X_train_scaled = scaler.transform(X_train_poly)\n",
    "    X_test_scaled = scaler.transform(X_test_poly)\n",
    "\n",
    "    lr = LinearRegression().fit(X_train_scaled, Y_train)\n",
    "    prediction_train = lr.predict(X_train_scaled)\n",
    "    prediction_test = lr.predict(X_test_scaled)\n",
    "# Compute and print RMSE using your code above on the training set and on the test set\n",
    "    RMSE_train = rmse(Y_train, prediction_train)\n",
    "    RMSE_test = rmse(Y_test, prediction_test)\n",
    "    print(\"For degree {}, the RMSE for the training set is {} and for the test set is {}\".format(power, RMSE_train, RMSE_test))\n",
    "# Compute and print R^2 on the training set and on the test set\n",
    "    r2_train = r2_score(Y_train, prediction_train)\n",
    "    r2_test = r2_score(Y_test, prediction_test)\n",
    "    print(\"For degree {}, the R^2 score for the training set is {} and for the test set is {}\".format(power, r2_train, r2_test))\n",
    "# Generate a plot with the x-axis representing the complexity of the model (i.e. the degree of the polynomial features)\n",
    "# Make the degree range from 1 to 5. The y-axis should represent the RMSE. Plot a line for degree = 1, 2, 3, 4, and 5\n",
    "# for the training error and the test error.\n",
    "    RMSE_scores_train.append(RMSE_train)\n",
    "    RMSE_scores_test.append(RMSE_test)\n",
    "# print(\"Train: {} \\nTest: {}\".format(RMSE_scores_train, RMSE_scores_test))\n",
    "plt.plot(RMSE_scores_train)\n",
    "plt.plot(RMSE_scores_test)\n",
    "plt.xlabel('Degree')\n",
    "plt.ylabel('RMSE')\n",
    "plt.xticks([0, 1, 2, 3, 4], [1, 2, 3, 4, 5])\n",
    "    \n",
    "plt.legend(['RMSE train', 'RMSE test'], loc = 'upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VLG8k-ajdZqW"
   },
   "source": [
    "<span style=\"color:blue\">**Yes, the models with increasing complexity overfit the data. The RMSE for the training set remains quite constant while the RMSE for the test data grows exponentially with the degree, indicating that the models with a higher degree overfit the training data. Therefore, when increasing the complexity, at a certain point it will hinder the performance of the model since it will start overfitting the data.**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3hdgIABvdZqY",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6c5dc39115a9346c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Part 5. Regularization [2pts]\n",
    "\n",
    "1. Feature selection using regularization\n",
    "2. Training/validation/test split\n",
    "3. Find the optimal parameter for the regularizer\n",
    "4. Compare linear regression with and without regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6a938bf9f7024e22",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Question 5a:\n",
    "Create a function that applies regularization, choose the \\lambda parameter (i.e. the alpha in python) equal to 500."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-cb499aece0330317",
     "locked": false,
     "schema_version": 1,
     "solution": true
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "def regularization(X, Y, lambd = 500):\n",
    "    \n",
    "    ### BEGIN SOLUTION\n",
    "    model = Lasso(lambd).fit(X, Y)\n",
    "    ### END SOLUTION\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kczALy-8dZqZ",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3f69bcc72259b460",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Question 5b:\n",
    "Compare the performance of the linear regression with and without regularization. Use the 2 degree polynomial features constructed at question 4b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "sH5XWcISdZqZ",
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-039145c67e2bd279",
     "locked": false,
     "points": 0.5,
     "schema_version": 1,
     "solution": true
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 932398179.4749913, tolerance: 382432.92882740335\n",
      "  positive)\n"
     ]
    }
   ],
   "source": [
    "# Apply the function from 5a for feature selection\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "# add polynomial features\n",
    "X_train_poly_2 = polynomial(X_train, 2)\n",
    "X_test_poly_2 = polynomial(X_test, 2)\n",
    "# train regularization model\n",
    "reg = regularization(X_train_poly_2, Y_train)\n",
    "Y_train_predicted_2 = reg.predict(X_train_poly_2)\n",
    "Y_test_predicted_2 = reg.predict(X_test_poly_2)\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE training with lasso regularization: 1649.0098155352405\n",
      "RMSE test with lasso regularization: 2058.346387606444\n",
      "RMSE training without lasso regularization: 1561.0209386553909\n",
      "RMSE test without lasso regularization: 10335.3158214917\n"
     ]
    }
   ],
   "source": [
    "# Compare the training and test RMSE of the linear regression with and without regularization\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "lr = LinearRegression().fit(X_train_poly_2, Y_train)\n",
    "prediction_train = lr.predict(X_train_poly_2)\n",
    "prediction_test = lr.predict(X_test_poly_2)\n",
    "RMSE_train_lr = mean_squared_error(Y_train, prediction_train)**0.5\n",
    "RMSE_test_lr = mean_squared_error(Y_test, prediction_test)**0.5\n",
    "RMSE_train_lasso = mean_squared_error(Y_train, Y_train_predicted_2)**0.5\n",
    "RMSE_test_lasso = mean_squared_error(Y_test, Y_test_predicted_2)**0.5\n",
    "print(\"RMSE training with lasso regularization: {}\\nRMSE test with lasso regularization: {}\\nRMSE training without lasso regularization: {}\\nRMSE test without lasso regularization: {}\"\n",
    "     .format(RMSE_train_lasso, RMSE_test_lasso, RMSE_train_lr, RMSE_test_lr))\n",
    "\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 features selected out of 90\n"
     ]
    }
   ],
   "source": [
    "# Identify how many features were selected (i.e. had a non-zero parameter \\theta) by the regularized model\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "\n",
    "# plt.figure(figsize=(10, 10))\n",
    "count = 0\n",
    "for i in reg.coef_:\n",
    "    if i != 0:\n",
    "        count+=1\n",
    "print(\"{} features selected out of {}\".format(count, len(reg.coef_)))\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nLBWaLc9dZqg"
   },
   "source": [
    "<span style=\"color:blue\">**Out of the 90 features, 63 features were selected. **</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-cff313fb66d82eac",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Question 5c:\n",
    "Compare the coefficients (i.e. the parameters \\theta) of the linear regression models with and without regularization.\n",
    "Plot them in a graph, where the x-axis is the index of a coefficient while the y-axis the magnitute of it. \n",
    "What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "k2ovwz0PdZql",
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-2e1e400fa5995203",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1a217b9f50>"
      ]
     },
     "execution_count": 496,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the graph here\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "coef_no_reg = lr.coef_[0]\n",
    "coef_with_reg = reg.coef_\n",
    "# plt.plot(coef_no_reg, coef_with_reg)\n",
    "plt.plot(coef_with_reg)\n",
    "plt.plot(coef_no_reg)\n",
    "\n",
    "plt.axis(xmin = 0, xmax = len(coef_no_reg))\n",
    "plt.legend(['With lasso regularization', 'Without regularization'], loc = 'upper left')\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IMJ9JOTNdZq0",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-61098ba6c5ba9428",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<span style=\"color:blue\">**The coefficients of the features have been minimized, leaving an almost straight line compared to the magnitude of coefficients without regularization. **</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Hfx8ACDndZq1",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-da1188e07c24903f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Part 6: Categorical features [3 pts]\n",
    "\n",
    "Load again the entire dataset, without selecting specific features as you did in Part 1. The dataset now contains both *numerical* and *categorical* features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(csv_path, sep=',', header = 0,\n",
    "                )\n",
    "\n",
    "y = df['nonViolPerPop'].values.reshape(-1,1)\n",
    "df.dropna(subset=['nonViolPerPop'],axis=0, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6a:\n",
    "Look at the description of the features. Which of the features are or should be treated as categorical? Add the column names in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "\n",
    "categorical_features = ['State', 'countyCode']\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6b [Research]:\n",
    "Impute values using the median, on the entire dataset (regardless of train/test split). \n",
    "Does it make sense to impute values for all missing features? If yes, do so. Otherwise explain and drop rows containing missing values you can't replace.\n",
    "\n",
    "Hint: If you want to impute missing values only for some specific columns, you can refer to this link: https://stackoverflow.com/questions/38584184/imputer-on-some-dataframe-columns-in-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.10000e+00, 2.14400e+01, 1.13300e+01, ..., 1.39459e+03,\n",
       "        7.51220e+04, 1.19800e+04],\n",
       "       [2.82000e+00, 2.13000e+01, 1.71800e+01, ..., 1.95595e+03,\n",
       "        3.68595e+04, 2.31230e+04],\n",
       "       [2.76000e+00, 4.05300e+01, 1.26500e+01, ..., 9.98879e+03,\n",
       "        3.68595e+04, 2.06590e+04],\n",
       "       ...,\n",
       "       [2.54000e+00, 2.77200e+01, 1.33900e+01, ..., 5.14494e+03,\n",
       "        3.57230e+04, 2.06590e+04],\n",
       "       [2.56000e+00, 2.40300e+01, 1.29900e+01, ..., 3.81793e+03,\n",
       "        2.24070e+04, 1.31310e+04],\n",
       "       [2.57000e+00, 2.49000e+01, 1.53300e+01, ..., 1.99298e+03,\n",
       "        3.76640e+04, 1.05670e+04]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### BEGIN SOLUTION\n",
    "df_categorical = df[categorical_features]\n",
    "df.drop(categorical_features, axis=1, inplace=True)\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy=\"median\" )\n",
    "df=imp.fit_transform(df)\n",
    "display(df)\n",
    "X = df.copy()\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">**No, it does not make sense to impute values for categorical features since there is no mean or median 'State' or 'countyCode' and this would not be helpful in a case like this. **</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6c:\n",
    "Have another look at the data, specifically the two new added features. Do you consider them useful/meaningful predictors for a (linear) Logistic Regression model? Explain.\n",
    "\n",
    "Can you think of doing something that can help a linear model take advantage of some additional information? If so, implement your solution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "df_categorical['countyCode'] = df_categorical['State'] + (df_categorical['countyCode'].apply(str))\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">**Yes, the states indicate a region within the country, which could affect the crime rate. This also counts for the county codes, which are a specific region within a state. For this reason we decided to make the county codes unique for a region by altering the column to [<State>+<countyCode>]. This way we can use the specific regions to train our model and make predictions for other data points within that particular region. If for example in a certain city the crime rate is high, then another place (close that city) with the same state +  county code combination is also likely to have a higher crime rate. This could be predicted through the state + county code identifier, which we created. \n",
    "**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6d:\n",
    "One hot encode the categorical features, then merge them back into the original dataframe/array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.1 , 21.44, 11.33, ...,  0.  ,  0.  ,  0.  ],\n",
       "       [ 2.82, 21.3 , 17.18, ...,  0.  ,  0.  ,  0.  ],\n",
       "       [ 2.76, 40.53, 12.65, ...,  0.  ,  0.  ,  0.  ],\n",
       "       ...,\n",
       "       [ 2.54, 27.72, 13.39, ...,  0.  ,  0.  ,  0.  ],\n",
       "       [ 2.56, 24.03, 12.99, ...,  0.  ,  0.  ,  0.  ],\n",
       "       [ 2.57, 24.9 , 15.33, ...,  0.  ,  0.  ,  0.  ]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### BEGIN SOLUTION\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "df_categorical['countyCode'] = df_categorical['countyCode']\n",
    "df_categorical = pd.get_dummies(df_categorical).values\n",
    "df = np.concatenate((df,df_categorical),axis=1)\n",
    "display(df)\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6e:\n",
    "Follow the instructions in the cell below. Due to randomization of the train_test_split, results might change. Make sure to **run the same code a few times** and understand what is happening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized: \n",
      "RMSE train: 707.4057934771213 \n",
      "RMSE val: 612.7511669481628 \n",
      "RMSE test: 765.3810616458713 \n",
      "\n",
      "Ridge regression with alpha = 0.0: \n",
      "RMSE train: 655.054041321184 \n",
      "RMSE val: 916.7243752001393 \n",
      "\n",
      "Ridge regression with alpha = 1e-15: \n",
      "RMSE train: 655.0540413211846 \n",
      "RMSE val: 916.7243652891342 \n",
      "\n",
      "Ridge regression with alpha = 1e-08: \n",
      "RMSE train: 656.1989321581855 \n",
      "RMSE val: 899.2235570133371 \n",
      "\n",
      "Ridge regression with alpha = 1e-05: \n",
      "RMSE train: 764.0806523082731 \n",
      "RMSE val: 865.641090709229 \n",
      "\n",
      "Ridge regression with alpha = 0.0001: \n",
      "RMSE train: 909.6505991533027 \n",
      "RMSE val: 1140.899775927763 \n",
      "\n",
      "Ridge regression with alpha = 0.001: \n",
      "RMSE train: 981.4875809453073 \n",
      "RMSE val: 1300.6366949444475 \n",
      "\n",
      "Ridge regression with alpha = 0.01: \n",
      "RMSE train: 994.8110747743674 \n",
      "RMSE val: 1354.26101715777 \n",
      "\n",
      "Ridge regression with alpha = 0.05: \n",
      "RMSE train: 1020.2806545964797 \n",
      "RMSE val: 1473.835035198614 \n",
      "\n",
      "Ridge regression with alpha = 0.1: \n",
      "RMSE train: 1071.8997433549168 \n",
      "RMSE val: 1602.10655525658 \n",
      "\n",
      "Ridge regression with alpha = 1.0: \n",
      "RMSE train: 1627.2393793787444 \n",
      "RMSE val: 2376.834220084091 \n",
      "\n",
      "Ridge regression with alpha = 10.0: \n",
      "RMSE train: 2050.924214119713 \n",
      "RMSE val: 2827.180511798074 \n",
      "\n",
      "The RMSE on the test set is the best for alpha = 1e-8 \n",
      "\n",
      "RMSE train with regularization: 656.1989321581855 vs. without regularization: 707.4057934771213 \n",
      "RMSE val with regularization: 899.2235570133371 vs. without regularization: 612.7511669481628 \n",
      "RMSE test with regularization: 800.5057391688222 vs. without regularization: 765.3810616458713 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler, Normalizer\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Split the dataset into training, validation and test sets:\n",
    "# Training set: 10% of the entire set\n",
    "# Make the remaining 10% a validation dataset and the rest a test set.\n",
    "\n",
    "### BEGIN SOLUTION ###\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.9)\n",
    "\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.1)\n",
    "\n",
    "### END SOLUTION ###\n",
    "\n",
    "\n",
    "# Normalize all features based on the training and validation set combined.\n",
    "\n",
    "### BEGIN SOLUTION ###\n",
    "transformer = Normalizer().fit(np.concatenate((X_train, X_val)))\n",
    "X_train = transformer.transform(X_train)\n",
    "X_val = transformer.transform(X_val)\n",
    "X_test = transformer.transform(X_test)\n",
    "### END SOLUTION ###\n",
    "\n",
    "\n",
    "# Train a linear regression model on the training and validation set combined. \n",
    "# Calculate the error on the training set and the test set.\n",
    "\n",
    "### BEGIN SOLUTION ###\n",
    "# train model\n",
    "lr = LinearRegression().fit(np.concatenate((X_train, X_val)), np.concatenate((y_train, y_val)))\n",
    "# predict\n",
    "prediction_train = lr.predict(X_train)\n",
    "prediction_val = lr.predict(X_val)\n",
    "prediction_test = lr.predict(X_test)\n",
    "# calculate rmse\n",
    "RMSE_train_lr = mean_squared_error(y_train, prediction_train)**0.5\n",
    "RMSE_val_lr = mean_squared_error(y_val, prediction_val)**0.5\n",
    "RMSE_test_lr = mean_squared_error(y_test, prediction_test)**0.5\n",
    "print('Normalized: \\nRMSE train: {} \\nRMSE val: {} \\nRMSE test: {} \\n'.format(RMSE_train_lr, RMSE_val_lr, RMSE_test_lr))\n",
    "### END SOLUTION ###\n",
    "\n",
    "\n",
    "# Train a regularized model (use Ridge regularization). To decide the parameter alpha, use a range of values to train,\n",
    "# the model. Test its performance on the validation set. Output the RMSE for different values of alpha, and choose the\n",
    "# best value.\n",
    "\n",
    "### BEGIN SOLUTION ###\n",
    "def train_ridge(alpha):\n",
    "    # train model\n",
    "    lr = Ridge(alpha=alpha).fit(X_train, y_train)\n",
    "    # predict\n",
    "    prediction_train = lr.predict(X_train)\n",
    "    prediction_val = lr.predict(X_val)\n",
    "    # calculate rmse\n",
    "    RMSE_train = mean_squared_error(y_train, prediction_train)**0.5\n",
    "    RMSE_val = mean_squared_error(y_val, prediction_val)**0.5\n",
    "#     RMSE_test = mean_squared_error(y_test, prediction_test)**0.5\n",
    "    print('Ridge regression with alpha = {}: \\nRMSE train: {} \\nRMSE val: {} \\n'.format(alpha, RMSE_train, RMSE_val))\n",
    "    return {'RMSE_train': RMSE_train, 'RMSE_val': RMSE_val}\n",
    "\n",
    "for alpha in [0.0, 1e-15, 1e-8, 0.00001, 0.0001, 0.001, 0.01, 0.05, 0.1, 1.0, 10.0]:\n",
    "    train_ridge(alpha)\n",
    "print(\"The RMSE on the test set is the best for alpha = 1e-8 \\n\")\n",
    "### END SOLUTION ###\n",
    "\n",
    "\n",
    "\n",
    "# Train the regularized model (use Ridge regularization) with the best alpha parameter found above on the traing and \n",
    "# validation set combined.\n",
    "\n",
    "### BEGIN SOLUTION ###\n",
    "# train model with alpha = 0.1\n",
    "lr = Ridge(alpha=1e-8).fit(X_train, y_train)\n",
    "\n",
    "### END SOLUTION ###\n",
    "\n",
    "\n",
    "# Compare the performance of the linear regression with and without regularization\n",
    "# predict\n",
    "prediction_train = lr.predict(X_train)\n",
    "prediction_val = lr.predict(X_val)\n",
    "prediction_test = lr.predict(X_test)\n",
    "# calculate rmse\n",
    "RMSE_train = mean_squared_error(y_train, prediction_train)**0.5\n",
    "RMSE_val = mean_squared_error(y_val, prediction_val)**0.5\n",
    "RMSE_test = mean_squared_error(y_test, prediction_test)**0.5\n",
    "print(\"\"\"RMSE train with regularization: {} vs. without regularization: {} \\nRMSE val with regularization: {} vs. without regularization: {} \\nRMSE test with regularization: {} vs. without regularization: {} \\n\"\"\"\n",
    "      .format(RMSE_train, RMSE_train_lr, RMSE_val, RMSE_val_lr, RMSE_test, RMSE_test_lr))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "23xaVneOdZrC",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a93fd8e5829ee29a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Question 6f:\n",
    "What are your conclusions for the two models compared? Which model should you choose over the two? Is hyperparameter tuning useful in this case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X_L6IfD2dZrC",
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-5d199066f44b9fd5",
     "locked": false,
     "points": 0.5,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "<span style=\"color:blue\">**After hyperparameter tuning we managed for the ridge regression model to approach the RMSE values for the linear regression model. However the linear model often beats the ridge regression model in this case, and most of the time the two RMSE values are very close bound. In this case we would choose the linear regression model, since most of the times it outperforms the ridge regression model. We have ran the two models many times with different alpha-values for ridge regression, which lead us to choose the alpha-value of 1e-8. Even though this alpha usually seems to score the best for ridge regression, most of the time linear regression outperforms the model.**</span>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "colab": {
   "default_view": {},
   "name": "Practical Assignment 1.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
